{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'domain',\n",
    "            'html',\n",
    "            ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '')]\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates()\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "    \n",
    "        nulls = dataframe['domain'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'domain' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['domain'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        # pegar somente o body do HTML\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_html_for_how_many_values(text):\n",
    "    try:              \n",
    "        regex_valores = re.compile(r'\\d+(?:\\.\\d{3})*(?:,\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        valores = regex_valores.findall(text)\n",
    "        return len(valores)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for values.\\nError:\\n' + str(e))\n",
    "\n",
    "def get_html_body(html_str):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_str, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while trying to get HTML body.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_links(row):\n",
    "    try:\n",
    "        from scrapy.http import HtmlResponse\n",
    "        from scrapy.linkextractors import LinkExtractor\n",
    "        \n",
    "        # Definindo os parâmetros do LinkExtractor\n",
    "        allowed_domains = []\n",
    "        tags = ['a', 'area']\n",
    "        attrs = ['href',]\n",
    "        link_extractor = LinkExtractor(allow_domains=allowed_domains, tags=tags, attrs=attrs, unique=False)\n",
    "\n",
    "        html = row['html']\n",
    "        domain = row['domain']\n",
    "        response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "        links = link_extractor.extract_links(response)\n",
    "\n",
    "        return [link.url for link in links]\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while searching for links in HTML.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoreference_links_from_html(row):\n",
    "    from scrapy.http import HtmlResponse\n",
    "    from scrapy.linkextractors import LinkExtractor\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "    \n",
    "    html = row['html']\n",
    "    domain = row['domain']\n",
    "    parsed_domain = urlparse(domain).netloc  # Parse the domain to get the netloc part\n",
    "\n",
    "    response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "    link_extractor = LinkExtractor(tags=['a', 'area'], attrs=['href'], unique=False)\n",
    "    links = link_extractor.extract_links(response)\n",
    "\n",
    "    autoreference_links = []\n",
    "    for link in links:\n",
    "        link_url = urlparse(link.url)\n",
    "        # Check if the link is a relative link or it belongs to the same domain\n",
    "        if not link_url.netloc or link_url.netloc == parsed_domain:\n",
    "            # Resolve relative link to absolute URL\n",
    "            full_url = urljoin(domain, link.url)\n",
    "            autoreference_links.append(full_url)\n",
    "\n",
    "    return autoreference_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataframe(dataframe, aditional_columns):\n",
    "    try:\n",
    "        # Colunas necessárias para o modelo\n",
    "        feature_columns = []\n",
    "\n",
    "        for columns in aditional_columns:\n",
    "            feature_columns += columns\n",
    "        df_features = dataframe.loc[:, feature_columns]\n",
    "        \n",
    "        return df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while trying to build features DataFrame.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()    \n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)    \n",
    "        dataframe.loc[:, 'tokens'] = dataframe.loc[:, 'html'].apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "        # dataframe.loc[:, 'html_size'] = dataframe.loc[:, 'html'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens'] = dataframe.loc[:, 'tokens'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens_unicos'] = dataframe.loc[:, 'tokens'].apply(lambda x: len(set(x)))\n",
    "\n",
    "        # dataframe.loc[:, 'autoreference_links'] = dataframe.apply(get_autoreference_links_from_html, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_autoreference_links'] = dataframe.loc[:, 'autoreference_links'].apply(len)\n",
    "\n",
    "        # dataframe.loc[:, 'links'] = dataframe.apply(get_html_links, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_links'] = dataframe.loc[:, 'links'].apply(len)\n",
    "\n",
    "        dataframe.loc[:, 'processed_cnpjs'] = dataframe.loc[:, 'html'].apply(extract_and_process_cnpjs)\n",
    "        dataframe.loc[:, 'has_cnpj'] = dataframe.loc[:, 'processed_cnpjs'].apply(bool)\n",
    "\n",
    "        dataframe.loc[:, 'count_prices'] = html_body.apply(process_html_for_how_many_prices)\n",
    "        # #dataframe.loc[:, 'count_prices'] = dataframe.loc[:, 'prices'].apply(len)\n",
    "        dataframe['has_prices'] = dataframe['count_prices'] > 1\n",
    "\n",
    "        # dataframe.loc[:, 'count_values'] = html_body.apply(process_html_for_how_many_values)\n",
    "        # #dataframe.loc[:, 'count_values'] = dataframe.loc[:, 'values'].apply(len)\n",
    "        \n",
    "        # aditional_columns = [\n",
    "        #     ['true_ecommerce']\n",
    "        # ]\n",
    "        # df_features = get_features_dataframe(dataframe, aditional_columns)\n",
    "\n",
    "        return dataframe#, df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('../data/validated_test_samples.parquet').rename(columns={'website': 'domain', 'is_true_ecommerce': 'true_ecommerce'}).reset_index(drop=True)\n",
    "# df = pd.read_parquet('../data/validated_data_pagarme.parquet').rename(columns={'host': 'domain', 'is_true_ecommerce': 'true_ecommerce'}).reset_index(drop=True)\n",
    "df = pd.read_parquet('../data/validated_data_pagarme_lista8_plataformas.parquet').rename(columns={'is_true_ecommerce': 'true_ecommerce'}).reset_index(drop=True)\n",
    "# df = pd.read_parquet('../data/validated_data_pagarme_non_ecomm_subset.parquet').rename(columns={'host': 'domain'}).reset_index(drop=True)\n",
    "\n",
    "df['html'] = df['html'].astype(str)\n",
    "df_copy = df.copy()\n",
    "\n",
    "df = df[['domain', 'html']]\n",
    "df = generate_features(df)\n",
    "indices = df.index.tolist()\n",
    "\n",
    "df_y = df_copy[df_copy.index.isin(indices)]['true_ecommerce']\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "features = ['has_cnpj', 'has_prices']\n",
    "other_features = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.6\n",
    "\n",
    "def predict_proba_with_domain(X, estimator, vectorizer, threshold=0.5):\n",
    "    model_predictions_prob = estimator.predict_proba(X)\n",
    "\n",
    "    y_probs_0, y_probs_1 = zip(*model_predictions_prob)\n",
    "    y_probs_0 = list(y_probs_0)\n",
    "    y_probs_1 = list(y_probs_1)\n",
    "    y_preds = np.asarray(list(map(lambda x: int(x >= threshold), y_probs_1)))\n",
    "\n",
    "    return y_preds, y_probs_0, y_probs_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using last version model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model_path = \"../models/MODEL_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl\"\n",
    "last_vectorizer_path = \"../models/VECTORIZER_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl\"\n",
    "\n",
    "with open(last_vectorizer_path, \"rb\") as f:\n",
    "    last_version_vectorizer = pickle.load(f)\n",
    "\n",
    "with open(last_model_path, \"rb\") as f:\n",
    "    last_version_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_matrix = last_version_vectorizer.transform(token_strings)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=last_version_vectorizer.get_feature_names_out())\n",
    "\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain(\n",
    "    features_df,\n",
    "    last_version_model,\n",
    "    last_version_vectorizer,\n",
    "    threshold=THRESHOLD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Accuracy: {round(accuracy_score(y_true=df_y, y_pred=y_preds), 4)}\")\n",
    "pprint(f\"F1-Score: {round(f1_score(y_true=df_y, y_pred=y_preds), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(classification_report(y_true=df_y, y_pred=y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_last = confusion_matrix(y_true=df_y, y_pred=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using current version model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_model_path = \"../models/MODEL_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_noisy_training_data.pkl\"\n",
    "# current_vectorizer_path = \"../models/VECTORIZER_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_noisy_training_data.pkl\"\n",
    "\n",
    "current_model_path = \"../models/MODEL_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_training_data.pkl\"\n",
    "current_vectorizer_path = \"../models/VECTORIZER_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_training_data.pkl\"\n",
    "\n",
    "# current_model_path = \"../models/MODEL_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_training_data_extra_samples.pkl\"\n",
    "# current_vectorizer_path = \"../models/VECTORIZER_v1_ecommerce_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_training_data_extra_samples.pkl\"\n",
    "\n",
    "with open(current_vectorizer_path, \"rb\") as f:\n",
    "    current_version_vectorizer = pickle.load(f)\n",
    "\n",
    "with open(current_model_path, \"rb\") as f:\n",
    "    current_version_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_matrix = current_version_vectorizer.transform(token_strings)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=current_version_vectorizer.get_feature_names_out())\n",
    "\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain(\n",
    "    features_df,\n",
    "    current_version_model,\n",
    "    current_version_vectorizer,\n",
    "    threshold=THRESHOLD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Accuracy: {round(accuracy_score(y_true=df_y, y_pred=y_preds), 4)}\")\n",
    "pprint(f\"F1-Score: {round(f1_score(y_true=df_y, y_pred=y_preds), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(classification_report(y_true=df_y, y_pred=y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_current = confusion_matrix(y_true=df_y, y_pred=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# Gera o heatmap da confusion matrix\n",
    "# plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm_last, \n",
    "            annot=True, \n",
    "            fmt=\"d\", \n",
    "            linewidths=.5, \n",
    "            square = True, \n",
    "            cmap = 'Blues', \n",
    "            annot_kws={\"size\": 16}, \n",
    "            xticklabels=['non_ecom', 'ecom'], \n",
    "            yticklabels=['non_ecom', 'ecom'], ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Modelo Atual', size = 20)\n",
    "# axs[0].set_xticks(rotation='horizontal', fontsize=16)\n",
    "# axs[0].set_yticks(rotation='horizontal', fontsize=16)\n",
    "axs[0].set_xlabel('Predicted Label', size=20)\n",
    "axs[0].set_ylabel('Actual Label', size=20)\n",
    "\n",
    "# Gera o heatmap da confusion matrix\n",
    "# plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm_current, \n",
    "            annot=True, \n",
    "            fmt=\"d\", \n",
    "            linewidths=.5, \n",
    "            square = True, \n",
    "            cmap = 'Blues', \n",
    "            annot_kws={\"size\": 16}, \n",
    "            xticklabels=['non_ecom', 'ecom'], \n",
    "            yticklabels=['non_ecom', 'ecom'], ax=axs[1])\n",
    "\n",
    "axs[1].set_title('Modelo Treinado com Dados Validados', size = 20)\n",
    "# axs[1].xticks(rotation='horizontal', fontsize=16)\n",
    "# axs[1].yticks(rotation='horizontal', fontsize=16)\n",
    "axs[1].set_xlabel('Predicted Label', size=20)\n",
    "axs[1].set_ylabel('Actual Label', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../images/16-04-2025/confusion_matrix_dados_analise_probabilidade_lista8.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
