{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import FloatType, BooleanType, StructField, StructType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_ENDPOINT_URL = os.getenv('AWS_ENDPOINT_URL')\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 20:01:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Spark com S3\").setMaster(\"local[*]\")\n",
    "\n",
    "conf.set(\"spark.driver.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"70g\")\n",
    "\n",
    "# conf.set(\"spark.driver.cores\", \"20\")\n",
    "# conf.set(\"spark.executor.cores\", \"20\")\n",
    "\n",
    "# conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "# conf.set(\"spark.memory.offHeap.size\", \"20g\")\n",
    "\n",
    "# conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "# conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \n",
    "conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = spark._jvm.org.apache.log4j\n",
    "# logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "# logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "# logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Inicialize o cliente boto3 para listar os objetos na pasta S3\n",
    "s3 = boto3.client('s3', endpoint_url='https://s3.bhs.io.cloud.ovh.net')\n",
    "bucket_name = 'drivalake'\n",
    "prefix = 'sites/bronze/spiderwebv4/countries_'\n",
    "\n",
    "# Função para listar todos os arquivos no bucket/prefix\n",
    "def list_s3_files(bucket, prefix):\n",
    "    files = []\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    for content in response.get('Contents', []):\n",
    "        files.append(content['Key'])\n",
    "    while 'NextContinuationToken' in response:\n",
    "        continuation_token = response['NextContinuationToken']\n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "        for content in response.get('Contents',  []):\n",
    "            files.append(content['Key'])\n",
    "    return files\n",
    "\n",
    "# Listar todos os arquivos\n",
    "files = list_s3_files(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(domains, htmls):\n",
    "    try:\n",
    "        # Transformar html para string\n",
    "        htmls = [str(html) for html in htmls]\n",
    "        \n",
    "        # Filtrar entradas\n",
    "        filtered_data = [(domain, html) for domain, html in zip(domains, htmls) \n",
    "                         if html != '[]' and html != '' and domain.endswith('.br')]\n",
    "        \n",
    "        if len(filtered_data) != len(domains):\n",
    "            count = len(domains) - len(filtered_data)\n",
    "            print(f\"WARNING: dataset has {count} entries with empty HTML and/or does not end with '.br'. Removing those entries.\")\n",
    "        \n",
    "        # Remover duplicatas\n",
    "        filtered_data = list(set(filtered_data))\n",
    "        if len(filtered_data) != len(domains):\n",
    "            count = len(domains) - len(filtered_data)\n",
    "            print(f\"WARNING: dataset has {count} entries with duplicate values. Removing those entries.\")\n",
    "        \n",
    "        # Remover valores nulos\n",
    "        filtered_data = [(domain, html) for domain, html in filtered_data if domain and html]\n",
    "        nulls = len(domains) - len(filtered_data)\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: dataset has {nulls} entries with null values. Removing those entries.\")\n",
    "        \n",
    "        # Separar as listas filtradas\n",
    "        filtered_domains, filtered_htmls = zip(*filtered_data) if filtered_data else ([], [])\n",
    "        \n",
    "        return list(filtered_domains), list(filtered_htmls)\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        # if os.path.exists(file_name):\n",
    "        #     os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens_lemmatized = []\n",
    "        for token in tokens:\n",
    "            if token in lemmatizer_pt_dict.keys():\n",
    "                tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "            else:\n",
    "                tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "        return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:\n",
    "        STOP_WORDS = set(stopwords.words('portuguese')).union(set(stopwords.words('english')))\n",
    "\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "        \n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "        \n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "        \n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "        tokens = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_how_many_prices(text):\n",
    "    import re\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_body(html_str):\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_str, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while trying to get HTML body.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(domains, htmls):\n",
    "    try:\n",
    "        domains, htmls = check_integrity(domains, htmls)\n",
    "        lem_dict = build_lemmatizer_pt_dict()\n",
    "        \n",
    "        html_bodies = [get_html_body(html) for html in htmls]\n",
    "        tokens_list = [process_html_for_vectorizer(html, lem_dict) for html in htmls]\n",
    "        \n",
    "        processed_cnpjs = [extract_and_process_cnpjs(html) for html in htmls]\n",
    "        has_cnpj = [bool(cnpjs) for cnpjs in processed_cnpjs]\n",
    "        \n",
    "        count_prices = [process_html_for_how_many_prices(html_body) for html_body in html_bodies]\n",
    "        has_prices = [count > 1 for count in count_prices]\n",
    "\n",
    "        return {\n",
    "            'domains': domains,\n",
    "            'htmls': htmls,\n",
    "            'tokens': tokens_list,\n",
    "            'processed_cnpjs': processed_cnpjs,\n",
    "            'has_cnpj': has_cnpj,\n",
    "            'count_prices': count_prices,\n",
    "            'has_prices': has_prices\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while generating features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def generate_features_without_pandas(domains, HTML_raw, vectorizer):\n",
    "    try:\n",
    "        lem_dict = build_lemmatizer_pt_dict()\n",
    "\n",
    "        # Processar cada par de domínio e HTML individualmente\n",
    "        features = []\n",
    "        for domain, html in zip(domains, HTML_raw):\n",
    "            tokens = process_html_for_vectorizer(html, lem_dict)\n",
    "            processed_cnpjs = extract_and_process_cnpjs(html)\n",
    "            has_cnpj = bool(processed_cnpjs)\n",
    "            has_prices = process_html_for_how_many_prices(html) > 0\n",
    "\n",
    "            features.append({\n",
    "                'tokens': tokens,\n",
    "                'has_cnpj': has_cnpj,\n",
    "                'has_prices': has_prices\n",
    "            })\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while generating features without Pandas.\\nError:\\n' + str(e))\n",
    "\n",
    "def predict_proba_with_domain(domains, HTML_raw, estimator, vectorizer):\n",
    "    try:\n",
    "        # Gerar as características a partir dos domínios e HTMLs\n",
    "        features = generate_features_without_pandas(domains, HTML_raw, vectorizer)\n",
    "\n",
    "        # Extrair tokens e transformar em strings para vetorização\n",
    "        token_strings = [' '.join(feature['tokens']) for feature in features]\n",
    "\n",
    "        # Transformar os tokens em matriz TF-IDF usando o vetorizador\n",
    "        tfidf_matrix = vectorizer.transform(token_strings)\n",
    "\n",
    "        # Obter características adicionais\n",
    "        has_cnpj = [feature['has_cnpj'] for feature in features]\n",
    "        has_prices = [feature['has_prices'] for feature in features]\n",
    "\n",
    "        # Criar DataFrame com as características adicionais\n",
    "        additional_features = pd.DataFrame({\n",
    "            'has_cnpj': has_cnpj,\n",
    "            'has_prices': has_prices\n",
    "        })\n",
    "\n",
    "        # Converter a matriz TF-IDF em DataFrame\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "        # Concatenar todas as características\n",
    "        combined_features = pd.concat([additional_features, tfidf_df], axis=1)\n",
    "\n",
    "        # Fazer a previsão de probabilidades usando o modelo\n",
    "        model_predictions_prob = estimator.predict_proba(combined_features)\n",
    "\n",
    "        # Extrair probabilidades das classes\n",
    "        y_probs_0 = model_predictions_prob[:, 0]\n",
    "        y_probs_1 = model_predictions_prob[:, 1]\n",
    "\n",
    "        # Gerar previsões baseadas nas probabilidades\n",
    "        y_preds = (y_probs_1 >= 0.5).astype(int)\n",
    "\n",
    "        return y_preds, y_probs_0, y_probs_1\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while predicting with domain.\\nError:\\n' + str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "Processing batch 1/127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "predicting...\n",
      "writing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 20:01:44 ERROR Executor: Exception in task 9.0 in stage 1.0 (TID 10): /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/22\n",
      "24/06/24 20:01:55 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/36/temp_shuffle_538ad78e-5bb3-414d-9566-e2aa675dfcb3\n",
      "24/06/24 20:01:55 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/33/temp_shuffle_19a9cc12-33a2-41a5-9797-76b45b753075\n",
      "24/06/24 20:01:55 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 4): /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/05\n",
      "24/06/24 20:01:55 ERROR Executor: Exception in task 16.0 in stage 1.0 (TID 17): /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/24\n",
      "24/06/24 20:01:57 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/29/temp_shuffle_56534bf2-dd36-4bd0-a269-ebeee45344b4\n",
      "24/06/24 20:01:57 ERROR Executor: Exception in task 24.0 in stage 1.0 (TID 25): /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/00\n",
      "24/06/24 20:02:09 ERROR Executor: Exception in task 33.0 in stage 1.0 (TID 34): /tmp/blockmgr-9212680d-b074-4f38-b25d-9ebc304d1372/21\n",
      "Current mem limits: -1 of max -1                                  (0 + 40) / 45]\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "\n",
      "/tmp/ipykernel_2519728/2901992765.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "Current mem limits: 75161927680 of max 75161927680                (0 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (1 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (2 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (4 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680\n",
      "\n",
      "ERROR:root:KeyboardInterrupt while sending command.              (25 + 20) / 45]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/darrazao/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/darrazao/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriting...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/ecommerce/countries_filtered_with_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mdf_with_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================>                   (29 + 16) / 45]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, BooleanType\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Pré-carregar o modelo e o vetor\n",
    "print('loading model...')\n",
    "with open('../models/MODEL_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\") as serialized_model:\n",
    "    model = pickle.load(serialized_model)\n",
    "\n",
    "with open('../models/VECTORIZER_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\") as serialized_vectorizer:\n",
    "    vectorizer = pickle.load(serialized_vectorizer)\n",
    "\n",
    "# Broadcast model and vectorizer to Spark executors\n",
    "broadcast_model = spark.sparkContext.broadcast(model)\n",
    "broadcast_vectorizer = spark.sparkContext.broadcast(vectorizer)\n",
    "\n",
    "# Obtenha os nomes das colunas de recursos\n",
    "feature_names = broadcast_vectorizer.value.get_feature_names_out()\n",
    "\n",
    "# Definir a função de predição fora do loop\n",
    "def predictor(domain, html):\n",
    "    y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain([domain], [html], broadcast_model.value, broadcast_vectorizer.value)\n",
    "    return float(y_probs_1[0]), bool(y_preds[0])\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"probability\", DoubleType()),\n",
    "    StructField(\"prediction\", BooleanType())\n",
    "])\n",
    "\n",
    "# Registrar a função UDF uma vez\n",
    "udf_predictor = udf(predictor, result_schema)\n",
    "\n",
    "batch_size = 5\n",
    "batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "\n",
    "# Carregar e processar cada parte separadamente\n",
    "for i, batch in enumerate(batches):\n",
    "    # if i < 8: continue # 74+\n",
    "\n",
    "    print(f\"Processing batch {i+1}/{len(batches)}\")\n",
    "    file_paths = [f\"s3a://{bucket_name}/{file}\" for file in batch]\n",
    "    df_spider_br = spark.read.parquet(*file_paths)\n",
    "    \n",
    "    # Fazer o processamento necessário com df_batch\n",
    "    print('preprocessing...')\n",
    "    df_spider_br = df_spider_br.select('domain', 'html', 'status')\n",
    "    df_spider_br = df_spider_br.withColumn('html', col('html').cast('string'))\n",
    "    df_spider_br = df_spider_br.filter((col('status') == 200.0) & \n",
    "                                       (col('html') != '[]') & \n",
    "                                       (col('html') != '') & \n",
    "                                       (col('domain').endswith('.br')))\n",
    "    df_spider_br = df_spider_br.select('domain', 'html')\n",
    "    df_spider_br = df_spider_br.dropDuplicates()\n",
    "\n",
    "    print('predicting...')\n",
    "    df_with_predictions = df_spider_br.withColumn('results', udf_predictor(df_spider_br.domain, df_spider_br.html))\n",
    "    \n",
    "    # Criar colunas separadas para probability e prediction\n",
    "    df_with_predictions = df_with_predictions.withColumn(\"probability\", col(\"results.probability\")) \\\n",
    "                                             .withColumn(\"prediction\", col(\"results.prediction\")) \\\n",
    "                                             .drop('results')\n",
    "\n",
    "    # Selecionar apenas as colunas necessárias\n",
    "    df_with_predictions = df_with_predictions.select('domain', 'html', 'probability', 'prediction')\n",
    "\n",
    "    print('writing...')\n",
    "    file_path = '../data/countries_filtered_with_predictions'\n",
    "    df_with_predictions.write.parquet(file_path, mode='append')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 20:08:09 ERROR FileFormatWriter: Aborting job 8a7b4b55-fd9b-4c72-866c-b919d094ed0f.\n",
      "org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 42.0 in stage 3.0 (TID 84): Connection reset\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 24.0 in stage 3.0 (TID 66): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 43.0 in stage 3.0 (TID 85): Connection reset\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 3.0 in stage 3.0 (TID 45): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 44.0 in stage 3.0 (TID 86): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 18.0 in stage 3.0 (TID 60): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 28.0 in stage 3.0 (TID 70): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/24 20:08:10 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 41.0 in stage 3.0 (TID 83): Broken pipe (Write failed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 21.0 in stage 3.0 (TID 63): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 30.0 in stage 3.0 (TID 72): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 16.0 in stage 3.0 (TID 58): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 12.0 in stage 3.0 (TID 54): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 20.0 in stage 3.0 (TID 62): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 6.0 in stage 3.0 (TID 48): Python worker exited unexpectedly (crashed)\n",
      "24/06/24 20:08:10 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n",
      "\t... 30 more\n",
      "24/06/24 20:08:10 WARN FileOutputCommitter: Could not delete file:/home/darrazao/git/accounting_website_classifier/data/ecommerce/countries_filtered_with_predictions/_temporary/0/_temporary/attempt_202406242002396946225687012052004_0003_m_000000_42\n",
      "24/06/24 20:08:10 ERROR FileFormatWriter: Job job_202406242002396946225687012052004_0003 aborted.\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 40.0 in stage 3.0 (TID 82): Connection reset\n",
      "24/06/24 20:08:10 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 42): [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/darrazao/git/accounting_website_classifier/data/ecommerce/countries_filtered_with_predictions.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spider_br.write.parquet('./data/spider_br/brazil_filtered.parquet', mode='error')\n",
    "# df_test = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')\n",
    "# df_spider_br = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
