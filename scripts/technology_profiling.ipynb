{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import FloatType, BooleanType, StructField, StructType, DoubleType, ArrayType\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_ENDPOINT_URL = os.getenv('AWS_ENDPOINT_URL')\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/07 13:05:54 WARN Utils: Your hostname, titan resolves to a loopback address: 127.0.1.1; using 139.99.60.146 instead (on interface eno1)\n",
      "24/08/07 13:05:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/xavier/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/xavier/.ivy2/cache\n",
      "The jars for the packages stored in: /home/xavier/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-29234c50-1886-4811-9f00-a94f5bf7a292;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 222ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-29234c50-1886-4811-9f00-a94f5bf7a292\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "24/08/07 13:05:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/07 13:05:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Spark com S3\").setMaster(\"local[*]\")\n",
    "\n",
    "conf.set(\"spark.driver.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"70g\")\n",
    "\n",
    "# conf.set(\"spark.driver.cores\", \"20\")\n",
    "# conf.set(\"spark.executor.cores\", \"20\")\n",
    "\n",
    "# conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "# conf.set(\"spark.memory.offHeap.size\", \"20g\")\n",
    "\n",
    "# conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "# conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \n",
    "conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('../data/ecommerce_technologies_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGENCIA', 'DEVHOUSE', 'ECCOSYS', 'KPL', 'SOFTVAR', 'FULL COMMERCE', 'GATEWAY', 'LOJA TESTE', 'MODO CALL CENTER', 'AG COMMERCE', 'AZTRONIC', 'BSELLER', 'BETALABS', 'BIS2BIS', 'BIZ COMMERCE', 'BLUEFOOT', 'COMM2', 'DIGITAL HUB', 'E-SMART', 'EZCOMMERCE', 'FBITS', 'FULL SOLUTIONS', 'GIRAN/ADENA', 'INNERSITE', 'JET ECOMMERCE', 'MAXIMAWEB', 'MOOVIN', 'NETFASHION', 'PRIMORDIA', 'SIGNATIVA', 'SMARTBILL', 'TATIX', 'TREZO', 'UECOMMERCE', 'VANNON', 'WEBJUMP', 'XTECH', 'SOFTWARE', '3 DOTS', '88 DIGITAL', 'ACCON', 'ADENA', 'AGILE COMMERCE', 'AGORA B2B', 'ALTEGIO', 'AMPLIA VENDAS', 'ANYSHOP', 'ASSINE STORE', 'AUREA', 'AXYSWEB', 'B4 COMMERCE', 'BATINGA CURSOS', 'BELL TECNOLOGIA', 'BETA SISTEMAS', 'BIG FISH', 'BIG SHOP', 'BRAAVO', 'BRAND ECOMMERCE', 'BRASIL NA WEB', 'BRING E-COMMERCE', 'BW COMMERCE', 'CAMMINO', 'CARTPANDA', 'CAZCO', 'CHEZ', 'CIASHOP', 'CLIMBA', 'COMMERCE PLUS', 'CONECTA LÁ', 'CONVERTIZE', 'CONVERTR', 'CRIAR LOJA ONLINE', 'DAWEB', 'DEZWORK', 'DIGI GIRLS', 'DIGITAL MANAGER GURU', 'DIGITAL SELLER', 'DLOJA VIRTUAL', 'DM COMMERCE', 'DOOCA', 'DOTTATEC', 'DUO STUDIO', 'DV9', 'DZAINE', 'E-BOX', 'E-FLIPS', 'ECOM CLUB', 'ECOM PLUS', 'ECOM-D', 'ECOMMERCE CAMP', 'ECOMMERCE FLEX', 'ECOMPLETO', 'EDOOLS', 'EDULABZZ', 'EDUZZ', 'EGONDOLA', 'ELEVAR', 'ELO IDEIAS', 'ENTREGA DIGITAL', 'F4TI', 'FACIL ZAP', 'FAST COMPRAS', 'FASTCOMMERCE', 'FITCOMMERCE', 'FLEXY', 'GET COMMERCE', 'GETCOURSE', 'GIRAFA COMUNICAÇÃO', 'GOUX', 'GRANSYS', 'HIGH COMMERCE', 'HISHOP', 'HUBSELL', 'ILION', 'ILURIA', 'IMPRIMA STORE', 'INBLESS', 'INOVARTI', 'ISET', 'JN2', 'KASTERWEB', 'KLOK', 'LEANCOMMERCE', 'LINX COMMERCE', 'LIRIX', 'LIVE ECOMMERCE', 'LOJ COMM', 'LOJA MESTRE', 'LOJA VIRTUAL', 'LOJAS VIRTUAIS BR', 'LOJCOMM', 'MACRO TECNOLOGIA', 'MAGAZORD', 'MAGENTO 1', 'MAGENTO 2', 'MAGESHOP', 'MAGEWORK', 'MALGA', 'MAXIMA ECOMMERCE', 'MEDPORTAL', 'MERCADO NS', 'MERCADO SHOPS', 'MESTRES DA WEB', 'MKX ECOMMERCE', 'MOBILE MIND', 'MOBLIX', 'N49', 'NEEMO', 'NEOGEST', 'NEOMODE', 'NEXAAS', 'NEXO DIGITAL', 'NITRO ECOM', 'NOVAOITO', 'OMNIBEES', 'OMNICHAT', 'OMNIK', 'OPENCART', 'OPENCART MASTER', 'OPENK', 'ORBITAL COMMERCE', 'ORUC', 'PERAS LOJA', 'PLUGG.TO', 'PORTAL DA MARIA', 'PRECODE', 'PRINT ONE', 'ROCKY ECOMMERCE', 'RP COMMERCE', 'RS PROJETOS', 'SAFARI', 'SHOPPUB', 'SHOWARE', 'SHOWCOMMERCE', 'SIGELOJA', 'SINTESE', 'SM PLACES', 'SOGNISPORTS', 'SOULMKT', 'SPIRIT SHOP', 'STA DICA', 'STARTUP SOLUCOES', 'STAYS', 'STOOM', 'SYSCOIN', 'THINKR', 'TINY', 'TOP LOJAS', 'TRICOMMERCE', 'TRIP PROPAGANDA', 'TUTOR', 'UOOU SOLUTIONS', 'URBIS', 'VENDA NA LIVE', 'VENDDOR', 'VENDIZAP', 'VIA SHOP MODA', 'VIDDY', 'VINDI', 'VISUAL ECOMMERCE', 'VNDA', 'VOWT', 'VOXEL DIGITAL', 'W2P SOLUTION', 'WAKE', 'WAPSTORE', 'WBUY', 'WD SHOP', 'WE BURN', 'WEBBA', 'WEBSTORE', 'WEBSTORM', 'WIDECOMMERCE', 'WIX', 'WOOCOMMERCE 1.0', 'WX3 ECOMMERCE', 'XCOMMERCE', 'YAMPI', 'YEPCOMM', 'YUNO', 'BUSCA MILHAS', 'LOJA QUE VENDE', 'BAGY', 'LAUNCH LMS', 'BRASPAG', 'GALAX PAY', 'PLUGA TECNOLOGIA', 'POOLPAY', 'ADD SUITE', 'CAFI B2B', 'DROPIM', 'FAGRON TECH', 'LINX COMMERCE GO', 'MEU DATACENTER', 'SPEED NOW', 'TRIBOX', 'BLUESOFT']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "technologies_df = spark.read.csv('../data/technologies.csv', header=True)\n",
    "technologies_list = [row.TECHNOLOGIES for row in technologies_df.collect()]\n",
    "\n",
    "exploded_df = df.select(explode(col('ecommerce_technology')).alias('technology'))\n",
    "\n",
    "distinct_technologies = [row.technology for row in exploded_df.select('technology').distinct().collect()]\n",
    "\n",
    "non_matching_technologies_list = [tech for tech in technologies_list if tech not in distinct_technologies]\n",
    "\n",
    "print(non_matching_technologies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muita divergência por conta de acentos, espaços e versionamento de tecnologias (ex: 'Magento 2' e 'Magento')\n",
    "len(non_matching_technologies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
