{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#http://ns5032832.ip-148-113-208.net:8888\n",
    "#xYT^5N5*KZoe4i\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import FloatType, BooleanType, StructField, StructType, DoubleType, ArrayType, StringType\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "conf = SparkConf().setAppName(\"Spark\").setMaster(\"local[*]\")\n",
    "\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_ENDPOINT_URL = \"\"\n",
    "\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"60g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"60g\")\n",
    "conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "conf.set(\"spark.jars\", \"/home/shared/drivers/postgresql-42.7.2.jar\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-Djavax.net.debug=all\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-Djavax.net.debug=all\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))\n",
    "print(spark._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import logging\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "\n",
    "# %%\n",
    "import boto3\n",
    "# Inicialize o cliente boto3 para listar os objetos na pasta S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name='bhs',\n",
    "    endpoint_url=AWS_ENDPOINT_URL,\n",
    ")\n",
    "bucket_name = 'drivalake'\n",
    "#prefix = 'sites/bronze/spiderwebv4/countries_'\n",
    "#prefix = 'spiderwebv4_output/pagarme_021224_crawler/'\n",
    "#prefix = 'sites/bronze/bodan_builtwith_27122024/'\n",
    "#prefix = 'raw/sites/spiderwebv4/cartpanda/year=2025/month=01/day=28/'\n",
    "#prefix = 'raw/sites/spiderwebv4/pagarme_probability/year=2025/month=01/'\n",
    "prefix = 'raw/sites/spiderwebv4/host_ecommerces/builtwith_pagarme/'\n",
    "\n",
    "# Função para listar todos os arquivos no bucket/prefix\n",
    "def list_s3_files(bucket, prefix):\n",
    "    files = []\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    for content in response.get('Contents', []):\n",
    "        files.append(content['Key'])\n",
    "    while 'NextContinuationToken' in response:\n",
    "        continuation_token = response['NextContinuationToken']\n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "        for content in response.get('Contents',  []):\n",
    "            files.append(content['Key'])\n",
    "    return files\n",
    "\n",
    "# Listar todos os arquivos\n",
    "files = list_s3_files(bucket_name, prefix)\n",
    "files = files[180:]\n",
    "\n",
    "print(files)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model helper functions\n",
    "\n",
    "# %%\n",
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'domain',\n",
    "            'html',\n",
    "        ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "        len_before_filter = dataframe.shape[0]\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '') & \n",
    "                                (dataframe['domain'].str.endswith('.br'))]\n",
    "        len_after_filter = dataframe.shape[0]\n",
    "\n",
    "        if len_before_filter != len_after_filter:\n",
    "            count = len_before_filter - len_after_filter\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates()\n",
    "        len_after_dropping_dup = dataframe.shape[0]\n",
    "\n",
    "        if len_after_filter != len_after_dropping_dup:\n",
    "            count = len_after_filter - len_after_dropping_dup\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "\n",
    "        nulls = dataframe['domain'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'domain' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['domain'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))\n",
    "\n",
    "# %%\n",
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"/home/shared/data-ops/website_classifier/data/lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"/home/shared/data-ops/website_classifier/data/lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"/home/shared/data-ops/website_classifier/data/lemmatization-pt.txt\"\n",
    "        # if os.path.exists(file_name):\n",
    "        #     os.remove(file_name)\n",
    "\n",
    "# %%\n",
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict, lemmatizer):\n",
    "    try:  \n",
    "      tokens_lemmatized = [\n",
    "          lemmatizer_pt_dict.get(token)\n",
    "          if token in lemmatizer_pt_dict.keys()\n",
    "          else lemmatizer.lemmatize(token)\n",
    "          for token in tokens\n",
    "      ]\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "# %%\n",
    "def get_html_body(html_str):\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''\n",
    "\n",
    "# %%\n",
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict, lemmatizer):\n",
    "    try:\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', html_text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # mantendo apenas as palavras cujo o tamanho é maior do que 2\n",
    "        tokens = list(filter(lambda x: len(x) > 2, tokens))\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token\n",
    "            for token in tokens\n",
    "            if token not in STOP_WORDS\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict, lemmatizer)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))\n",
    "\n",
    "# %%\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "# %%\n",
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_company(company_id):\n",
    "    company_id = only_number(company_id)\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    return company_id\n",
    "\n",
    "def order_by_common(data):\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = list(filter(lambda x: len(extract_company(x)) == 14, matches))\n",
    "    return processed_matches\n",
    "\n",
    "# %%\n",
    "def generate_features(dataframe, lem_dict, lemmatizer):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "       \n",
    "        dataframe.loc[:, 'processed_cnpjs'] = dataframe.loc[:, 'html'].apply(extract_and_process_cnpjs)\n",
    "        dataframe.loc[:, 'has_cnpj'] = dataframe.loc[:, 'processed_cnpjs'].apply(bool)\n",
    "\n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)\n",
    "        dataframe.loc[:, 'tokens'] = html_body.apply(lambda x: process_html_for_vectorizer(x, lem_dict, lemmatizer))\n",
    "        dataframe.loc[:, 'count_prices'] = html_body.apply(process_html_for_how_many_prices)\n",
    "        dataframe['has_prices'] = dataframe['count_prices'] > 1\n",
    "\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))\n",
    "\n",
    "# %%\n",
    "def predict_proba_with_domain(\n",
    "    domains: list,\n",
    "    HTML_raw: list,\n",
    "    estimator,\n",
    "    vectorizer,\n",
    "    lem_dict,\n",
    "    lemmatizer,\n",
    "):\n",
    "    df = pd.DataFrame({'domain': domains, 'html': HTML_raw})\n",
    "    df = generate_features(df, lem_dict, lemmatizer)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "    tfidf_matrix = vectorizer.transform(token_strings)\n",
    "\n",
    "    features = ['has_cnpj', 'has_prices']\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matrix.toarray(),\n",
    "        columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    other_features = df[features]\n",
    "    features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "    \n",
    "    model_predictions_prob = estimator.predict_proba(features_df)\n",
    "\n",
    "    y_probs_0, y_probs_1 = zip(*model_predictions_prob)\n",
    "    y_probs_0 = list(y_probs_0)\n",
    "    y_probs_1 = list(y_probs_1)\n",
    "    y_preds = list(map(lambda x: int(x >= 0.5), y_probs_1))\n",
    "\n",
    "    return y_preds, y_probs_0, y_probs_1\n",
    "\n",
    "# prediction method\n",
    "def predictor(domain, html):\n",
    "    y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain(\n",
    "        [domain], [html], model, vectorizer, lem_dict, lemmatizer\n",
    "    )\n",
    "    return (float(y_probs_1[0]), bool(y_preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# %% [markdown]\n",
    "# # Load model and process!\n",
    "\n",
    "# Load the CSV file containing the keys (adjust schema as needed)\n",
    "#df_keys = spark.read.option(\"header\", True).csv(\"/home/shared/data-ops/website_classifier/data/domains/domains_input.csv\")\n",
    "\n",
    "# Select the key column and drop duplicates\n",
    "key_column = \"dominio\"  # Change this to match your key column in the CSV\n",
    "#df_keys = df_keys.(col(key_column).cast(StringType())).dropDuplicates()\n",
    "\n",
    "print('loading model...')\n",
    "# Open picked model\n",
    "serialized_model = open('/home/shared/data-ops/website_classifier/models/ecomm/MODEL_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\")\n",
    "model = pickle.load(serialized_model)\n",
    "serialized_model.close()\n",
    "\n",
    "print('loading vectorizer...')\n",
    "# Open picked vectorizer\n",
    "serialized_vectorizer = open('/home/shared/data-ops/website_classifier/models/ecomm/VECTORIZER_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\")\n",
    "vectorizer = pickle.load(serialized_vectorizer)\n",
    "serialized_vectorizer.close()\n",
    "\n",
    "print('building lemmatizer pt dict...')\n",
    "lem_dict = build_lemmatizer_pt_dict() \n",
    "\n",
    "print('creating lemmatizer with wordnet...')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Broadcast model to spark executors\n",
    "spark.sparkContext.broadcast(model)\n",
    "spark.sparkContext.broadcast(vectorizer)\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"probability\", DoubleType()),\n",
    "    StructField(\"prediction\", BooleanType())\n",
    "])\n",
    "\n",
    "#register python method as spark UDF\n",
    "udf_predictor = udf(predictor, result_schema)\n",
    "\n",
    "# %%\n",
    "batch_size = 1\n",
    "batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "batches = batches[59:]\n",
    "batches = batches[:10]\n",
    "\n",
    "# Carregar e processar cada parte separadamente\n",
    "for i, batch in enumerate(tqdm(batches)):\n",
    "    #if i > 3: continue # 74+\n",
    "    \n",
    "    print(f\"Processing batch {i+1}/{len(batches)}\")\n",
    "    file_paths = [f\"s3a://{bucket_name}/{file}\" for file in batch]\n",
    "\n",
    "    print(f'reading {file_paths}')\n",
    "    df_spider_br = spark.read.parquet(*file_paths)#.limit(10000)\n",
    "    # df_spider_br.show()\n",
    "\n",
    "    # Fazer o processamento necessário com df_batch\n",
    "    print('preprocessing...')\n",
    "    df_spider_br = df_spider_br.select('domain', 'html', 'status')\n",
    "    df_spider_br = df_spider_br.withColumn('html', col('html').cast('string'))\n",
    "    df_spider_br = df_spider_br.filter((col('status') == 200.0) & (col('html') != '[]') & (col('html') != '') & (col('domain').endswith('.br')))\n",
    "    df_spider_br = df_spider_br.select('domain', 'html')\n",
    "    df_spider_br = df_spider_br.dropDuplicates()\n",
    "\n",
    "    # Fazer o predict com df_batch\n",
    "    print('predicting...')\n",
    "    df_with_predictions = df_spider_br.withColumn('results', udf_predictor(df_spider_br.domain, df_spider_br.html))\n",
    "\n",
    "    # Criar colunas separadas para probability e prediction\n",
    "    df_with_predictions = df_with_predictions.withColumn(\"probability\", col(\"results.probability\")) \\\n",
    "                                                .withColumn(\"prediction\", col(\"results.prediction\")) \\\n",
    "                                                .drop('results')\n",
    "    # df_with_predictions.show()\n",
    "\n",
    "    print('writing...')\n",
    "    file_path = f's3a://drivalake/trusted/sites/predictions_ecommerces_batched/builtwith_pagarme/part_{i}.parquet'\n",
    "    # file_path = f's3a://drivalake/sites/teste_ecom/part_{i}.parquet'\n",
    "    df_with_predictions.write.parquet(file_path, mode=\"overwrite\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# %%\n",
    "# df_spider_br.write.parquet('./data/spider_br/brazil_filtered.parquet', mode='error')\n",
    "# df_test = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')\n",
    "# df_spider_br = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
