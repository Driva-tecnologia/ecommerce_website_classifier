{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and df reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "OPENAI_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'domain',\n",
    "            'html',\n",
    "            ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '') & \n",
    "                                (dataframe['domain'].str.endswith('.br'))]\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates()\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "    \n",
    "        nulls = dataframe['domain'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'domain' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['domain'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_body(html_str):\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''\n",
    "    \n",
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        # pegar somente o body do HTML\n",
    "        text = get_html_body(html_text)\n",
    "        # soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        # text = soup.body.get_text() if soup.body else ''\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_html_for_how_many_values(text):\n",
    "    try:              \n",
    "        regex_valores = re.compile(r'\\d+(?:\\.\\d{3})*(?:,\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        valores = regex_valores.findall(text)\n",
    "        return len(valores)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for values.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_links(row):\n",
    "    try:\n",
    "        from scrapy.http import HtmlResponse\n",
    "        from scrapy.linkextractors import LinkExtractor\n",
    "        \n",
    "        # Definindo os parâmetros do LinkExtractor\n",
    "        allowed_domains = []\n",
    "        tags = ['a', 'area']\n",
    "        attrs = ['href',]\n",
    "        link_extractor = LinkExtractor(allow_domains=allowed_domains, tags=tags, attrs=attrs, unique=False)\n",
    "\n",
    "        html = row['html']\n",
    "        domain = row['domain']\n",
    "        response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "        links = link_extractor.extract_links(response)\n",
    "\n",
    "        return [link.url for link in links]\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while searching for links in HTML.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoreference_links_from_html(row):\n",
    "    from scrapy.http import HtmlResponse\n",
    "    from scrapy.linkextractors import LinkExtractor\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "    \n",
    "    html = row['html']\n",
    "    domain = row['domain']\n",
    "    parsed_domain = urlparse(domain).netloc  # Parse the domain to get the netloc part\n",
    "\n",
    "    response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "    link_extractor = LinkExtractor(tags=['a', 'area'], attrs=['href'], unique=False)\n",
    "    links = link_extractor.extract_links(response)\n",
    "\n",
    "    autoreference_links = []\n",
    "    for link in links:\n",
    "        link_url = urlparse(link.url)\n",
    "        # Check if the link is a relative link or it belongs to the same domain\n",
    "        if not link_url.netloc or link_url.netloc == parsed_domain:\n",
    "            # Resolve relative link to absolute URL\n",
    "            full_url = urljoin(domain, link.url)\n",
    "            autoreference_links.append(full_url)\n",
    "\n",
    "    return autoreference_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataframe(dataframe, aditional_columns):\n",
    "    try:\n",
    "        # Colunas necessárias para o modelo\n",
    "        feature_columns = []\n",
    "\n",
    "        for columns in aditional_columns:\n",
    "            feature_columns += columns\n",
    "        df_features = dataframe.loc[:, feature_columns]\n",
    "        \n",
    "        return df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while trying to build features DataFrame.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()    \n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)    \n",
    "        dataframe.loc[:, 'tokens'] = dataframe.loc[:, 'html'].apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "        # dataframe.loc[:, 'html_size'] = dataframe.loc[:, 'html'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens'] = dataframe.loc[:, 'tokens'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens_unicos'] = dataframe.loc[:, 'tokens'].apply(lambda x: len(set(x)))\n",
    "\n",
    "        # dataframe.loc[:, 'autoreference_links'] = dataframe.apply(get_autoreference_links_from_html, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_autoreference_links'] = dataframe.loc[:, 'autoreference_links'].apply(len)\n",
    "\n",
    "        # dataframe.loc[:, 'links'] = dataframe.apply(get_html_links, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_links'] = dataframe.loc[:, 'links'].apply(len)\n",
    "\n",
    "        dataframe.loc[:, 'processed_cnpjs'] = dataframe.loc[:, 'html'].apply(extract_and_process_cnpjs)\n",
    "        dataframe.loc[:, 'has_cnpj'] = dataframe.loc[:, 'processed_cnpjs'].apply(bool)\n",
    "\n",
    "        dataframe.loc[:, 'count_prices'] = html_body.apply(process_html_for_how_many_prices)\n",
    "        # #dataframe.loc[:, 'count_prices'] = dataframe.loc[:, 'prices'].apply(len)\n",
    "        dataframe['has_prices'] = dataframe['count_prices'] > 1\n",
    "\n",
    "        # dataframe.loc[:, 'count_values'] = html_body.apply(process_html_for_how_many_values)\n",
    "        # #dataframe.loc[:, 'count_values'] = dataframe.loc[:, 'values'].apply(len)\n",
    "        \n",
    "        # aditional_columns = [\n",
    "        #     ['true_ecommerce']\n",
    "        # ]\n",
    "        # df_features = get_features_dataframe(dataframe, aditional_columns)\n",
    "\n",
    "        return dataframe#, df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/filtered_data_samples_25k.parquet')\n",
    "df = df.rename(columns={'host': 'domain'})\n",
    "df['html'] = df['html'].astype(str)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_features(df)\n",
    "df = df.sample(5000, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "TEST_LEN = 250\n",
    "BACKUP_PATH = \"../data/sample_classified.csv\"\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_get(value):\n",
    "        return value if value not in [None, \"\"] else \"Não informado\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _output_parser(content):\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            json_string = match.group(0)\n",
    "            data = json.loads(json_string)\n",
    "            data = {k: data.get(k) for k in ['domain', 'is_ecommerce']}\n",
    "            return data\n",
    "        else:\n",
    "            print(\"No JSON found\")\n",
    "\n",
    "    def prompt(self, row: pd.Series):\n",
    "        return f\"\"\" \n",
    "        Objetivo:\n",
    "        Classificar um site como e-commerce ou não e-commerce, utilizando apenas os tokens extraídos do seu conteúdo HTML.\n",
    "\n",
    "        Definição de E-commerce:\n",
    "        Um site é considerado e-commerce somente se permitir a realização completa da compra de produtos ou serviços pela internet, incluindo todas as etapas abaixo:\n",
    "            1- Seleção de produtos ou serviços diretamente no site.\n",
    "            2- Adição dos itens a um carrinho de compras.\n",
    "            3- Escolha de forma de pagamento.\n",
    "            4- Definição de endereço de entrega ou retirada.\n",
    "            5- Finalização da compra com um processo de checkout online.\n",
    "\n",
    "        Importante:\n",
    "        Sites que apenas exibem produtos ou serviços, como catálogos, cardápios ou listas, sem permitir a compra direta no site, não devem ser considerados e-commerce, mesmo que exibam preços ou informações de contato.\n",
    "        Para que um site seja considerado e-commerce, ele deve conter todos os itens citados na definição do e-commerce.\n",
    "\n",
    "        Não caracterizam e-commerce:\n",
    "            1- Sites que apenas recebem pedidos por telefone, WhatsApp, formulário ou e-mail.\n",
    "            2- Sites com listas de produtos sem carrinho, sem checkout ou sem formas de pagamento online.\n",
    "            3- Cardápios de lanchonetes, padarias ou mercearias sem sistema de compra online.\n",
    "            4- Sites que apenas realizam o orçamento dos produtos e sem sistema de compra online.\n",
    "            5- Sites que não possuem carrinho ou que não permitam a compra dos produtos diretamente pelo site.\n",
    "\n",
    "        Entrada:\n",
    "        Uma lista de tokens extraídos do HTML do site. O URL/domain do site não deve ser usado como entrada para realizar a classificação, apenas na identificação do site na resposta.\n",
    "\n",
    "        Saída esperada:\n",
    "        Apenas um dos seguintes valores inteiros:\n",
    "            1: É um e-commerce (atende a todos os critérios listados).\n",
    "            0: Não é um e-commerce (não permite a compra diretamente no site).\n",
    "            -1: Indefinido (os tokens não são suficientes para concluir com confiança).\n",
    "        \n",
    "        Dados:\n",
    "        URL: {self._safe_get(row.get('domain'))}\n",
    "        Tokens: {self._safe_get(row.get('tokens'))}\n",
    "\n",
    "        Responda o 'domain' (URL) do site e 'is_ecommerce' (com o valor se ele é e-commerce ou não), em JSON.\n",
    "\n",
    "        Resposta:\n",
    "        \"\"\"\n",
    "    \n",
    "    def classify(self, row: pd.Series):\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.prompt(row)}\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "\n",
    "        data = self._output_parser(chat_completion.choices[0].message.content)\n",
    "        return data\n",
    "    \n",
    "        \n",
    "    async def parallel_classify(self, X: pd.DataFrame, batch_size: int=10, sleep=0.0):\n",
    "        \"\"\"Classifies data in batches asynchronously and yields it\"\"\"\n",
    "        \n",
    "        async_client = AsyncOpenAI(api_key=self.client.api_key)\n",
    "        all_results = {}\n",
    "\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X))\n",
    "            batch = X.iloc[batch_start:batch_end]\n",
    "\n",
    "            tasks = []\n",
    "            batch_indices = []\n",
    "\n",
    "            for idx, row in batch.iterrows():\n",
    "                prompt = [{'role': 'user', 'content': self.prompt(row)}]\n",
    "                task = async_client.chat.completions.create(\n",
    "                    messages=prompt,\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    temperature=0,\n",
    "                    max_tokens=100,\n",
    "                )\n",
    "                tasks.append(task)\n",
    "                batch_indices.append(idx)\n",
    "\n",
    "            # Execute batch asynchronously\n",
    "            batch_responses = await asyncio.gather(*tasks)\n",
    "\n",
    "            batch_results = {}\n",
    "            for idx, response in zip(batch_indices, batch_responses):\n",
    "                data = self._output_parser(response.choices[0].message.content)\n",
    "\n",
    "                batch_results[idx] = data\n",
    "                all_results[idx] = data\n",
    "\n",
    "            # Yield results per batch\n",
    "            yield pd.DataFrame.from_dict(batch_results, orient='index')\n",
    "\n",
    "            # Delay between batches\n",
    "            await asyncio.sleep(sleep)\n",
    "\n",
    "async def annotate(df, classifier, batch_size=50):\n",
    "    \"\"\"Yields batches of annotated data, including original DataFrame columns.\"\"\"\n",
    "    async for batch in classifier.parallel_classify(df, batch_size=batch_size):\n",
    "        # Create a DataFrame for the batch with original df index\n",
    "        annotated_batch = pd.DataFrame(batch, index=df.loc[batch.index].index)\n",
    "\n",
    "        yield annotated_batch\n",
    "\n",
    "def append_if_exists(df: pd.DataFrame, path: str):\n",
    "    \"\"\"Appends `df` to an existing file if it exists, otherwise creates a new file.\n",
    "    \n",
    "    Supports both CSV and Parquet formats. If a Parquet file is stored as a folder, it loads and appends correctly.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to append.\n",
    "        path (str): The file path, should end in .csv or .parquet.\n",
    "    \"\"\"\n",
    "    existing_df = load_file_if_exists(path)\n",
    "\n",
    "    if not existing_df is None:\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame in the correct format\n",
    "    if path.endswith(\".csv\"):\n",
    "        print('saved at', path)\n",
    "        df.to_csv(path, index=False, sep=';')\n",
    "    elif path.endswith(\".parquet\"):\n",
    "        df.to_parquet(path, index=False)  # Will save as a folder if partitioning is used\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .csv or .parquet\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_file_if_exists(path: str):\n",
    "    \"\"\"Checks and open a file if it exists.\n",
    "    \n",
    "    Supports both CSV and Parquet formats. If a Parquet file is stored as a folder, it loads and appends correctly.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path, should end in .csv or .parquet.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(path) or os.path.isdir(path)  # Check if it's a folder (Parquet case)\n",
    "\n",
    "    if file_exists:\n",
    "        if path.endswith(\".csv\"):\n",
    "            existing_df = pd.read_csv(path, sep=';')\n",
    "        elif path.endswith(\".parquet\"):\n",
    "            existing_df = pd.read_parquet(path)  # Reads the folder as a Parquet dataset\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .csv or .parquet\")\n",
    "        \n",
    "        return existing_df\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(api_key=OPENAI_KEY)\n",
    "samples = []\n",
    "df_copy = df.copy()\n",
    "existing_samples = load_file_if_exists(BACKUP_PATH)\n",
    "\n",
    "if not existing_samples is None:\n",
    "    # excluding the samples that already were processed\n",
    "    unique_domains = existing_samples[\"domain\"].unique().tolist()\n",
    "    df = df[~df[\"domain\"].isin(unique_domains)].reset_index(drop=True)\n",
    "\n",
    "async for batch in annotate(df, classifier, batch_size=BATCH_SIZE):\n",
    "    samples.append(batch)\n",
    "    append_if_exists(batch, BACKUP_PATH)  # Append in batches\n",
    "    print(f'{min(BATCH_SIZE, len(batch))} classified samples were added to {BACKUP_PATH}')\n",
    "\n",
    "labeled_df = pd.concat(samples, axis=0)\n",
    "labeled_df = labeled_df[labeled_df[\"is_ecommerce\"] != -1].reset_index(drop=True)\n",
    "\n",
    "if df.shape[0] != df_copy.shape[0]:\n",
    "    labeled_df = pd.merge(left=df_copy, right=labeled_df, how=\"inner\")\n",
    "else:\n",
    "    labeled_df = pd.merge(left=df, right=labeled_df, how=\"inner\")\n",
    "\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_df.to_parquet(\"../data/noisy_training_data.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
