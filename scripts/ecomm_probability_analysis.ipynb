{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-whois\n",
    "!pip install tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from glob import glob\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import tld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899bf0b",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aabd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"App\").setMaster(\"local[*]\")\n",
    "\n",
    "# Habilitar otimizações e configurações adicionais\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.ignore_timezone\", \"true\")\n",
    "\n",
    "# AWS S3 CONNECTION\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_ENDPOINT_URL = \"https://s3.bhs.io.cloud.ovh.net\"\n",
    "AWS_REGION = \"bhs\"\n",
    "\n",
    "conf.set(\"spark.driver.memory\", \"30g\")\n",
    "conf.set(\"spark.executor.memory\", \"30g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"30g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"30g\")\n",
    "conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "# conf.set(\"spark.jars\", \"/home/shared/drivers/postgresql-42.7.2.jar\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "# conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "# conf.set(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "# conf.set(\"spark.executor.extraJavaOptions\", \"-Djavax.net.debug=all\")\n",
    "# conf.set(\"spark.driver.extraJavaOptions\", \"-Djavax.net.debug=all\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))\n",
    "print(spark._jsc.sc().listJars())\n",
    "\n",
    "print(\"Spark session configurada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440ac3c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(site: str) -> bool:\n",
    "    res = tld.get_tld(site, fix_protocol=True, fail_silently=True)\n",
    "    return res is not None\n",
    "\n",
    "def extract_domain(site: str, raise_if_invalid=True) -> str:\n",
    "    if not validate(site):\n",
    "        if raise_if_invalid:\n",
    "            raise ValueError(\"Invalid site\")\n",
    "        return None\n",
    "    res = tld.get_tld(site, fix_protocol=True, as_object=True)\n",
    "    return res.fld\n",
    "\n",
    "def partition_number(x):\n",
    "    return int(x.split(\"_\")[-1].replace(\".parquet\", \"\"))\n",
    "\n",
    "def get_interval_range(probability):\n",
    "    probability = float(probability)\n",
    "\n",
    "    if probability < 0.1:\n",
    "        return '(0.0, 0.1)'\n",
    "    elif probability >= 0.1 and probability < 0.2:\n",
    "        return '(0.1, 0.2)'\n",
    "    elif probability >= 0.2 and probability < 0.3:\n",
    "        return '(0.2, 0.3)'\n",
    "    elif probability >= 0.3 and probability < 0.4:\n",
    "        return '(0.3, 0.4)'\n",
    "    elif probability >= 0.4 and probability < 0.5:\n",
    "        return '(0.4, 0.5)'\n",
    "    elif probability >= 0.5 and probability < 0.6:\n",
    "        return '(0.5, 0.6)'\n",
    "    elif probability >= 0.6 and probability < 0.7:\n",
    "        return '(0.6, 0.7)'\n",
    "    elif probability >= 0.7 and probability < 0.8:\n",
    "        return '(0.7, 0.8)'\n",
    "    elif probability >= 0.8 and probability < 0.9:\n",
    "        return '(0.8, 0.9)'\n",
    "    else:\n",
    "        return '(0.9, 1.0)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb0493",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798c23e",
   "metadata": {},
   "source": [
    "## Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/media/greca/HD/Driva/hosts.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062162eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"url\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitive_urls = df.groupBy(\"url\").count() \\\n",
    "                       .sort(col(\"count\").desc())\n",
    "repetitive_urls = repetitive_urls.filter(col(\"count\") > 1)\n",
    "repetitive_urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae316b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_repetitive_urls = df.groupBy(\"url\").count() \\\n",
    "                       .sort(col(\"count\").desc())\n",
    "non_repetitive_urls = non_repetitive_urls.filter(col(\"count\") == 1)\n",
    "non_repetitive_urls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"host\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1dc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitive_domains = df.groupBy(\"host\").count() \\\n",
    "                       .sort(col(\"count\").desc())\n",
    "repetitive_domains = repetitive_domains.filter(col(\"count\") > 1)\n",
    "repetitive_domains.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_repetitive_domains = df.groupBy(\"host\").count() \\\n",
    "                       .sort(col(\"count\").desc())\n",
    "non_repetitive_domains = non_repetitive_domains.filter(col(\"count\") == 1)\n",
    "non_repetitive_domains.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd15d",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.6\n",
    "\n",
    "files = glob(\"/media/greca/HD/Driva/smaller_ecommerce_whois_without_html/*.parquet\")\n",
    "files = sorted(files, key=partition_number)\n",
    "df = spark.read.parquet(*files)\n",
    "\n",
    "udf_mapping = udf(lambda z: get_interval_range(z), StringType()) \n",
    "\n",
    "higher_than_threshold = df.filter(col(\"probability\") >= THRESHOLD).count()\n",
    "lower_than_threshold = df.filter(col(\"probability\") < THRESHOLD).count()\n",
    "\n",
    "df = df.withColumn(\"interval\", udf_mapping(df.probability))\n",
    "\n",
    "total = df.groupBy(\"interval\").count().agg(F.sum(\"count\")).collect()[0][0]\n",
    "df = df.groupBy(\"interval\").count()\n",
    "df = df.withColumn(\"percent\", F.round(df['count']/total, 4))\n",
    "df = df.sort(col(\"percent\").desc())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_than_threshold, lower_than_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83447d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = higher_than_threshold + lower_than_threshold\n",
    "\n",
    "higher_than_threshold/total, lower_than_threshold/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa511a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"/media/greca/HD/Driva/smaller_ecommerce_whois_without_html/*.parquet\")\n",
    "files = sorted(files, key=partition_number)\n",
    "df = spark.read.parquet(*files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb563470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(site: str) -> bool:\n",
    "    res = tld.get_tld(site, fix_protocol=True, fail_silently=True)\n",
    "    return res is not None\n",
    "\n",
    "def extract_tld(site: str, raise_if_invalid=True) -> str:\n",
    "    if not validate(site):\n",
    "        if raise_if_invalid:\n",
    "            raise ValueError(\"Invalid site\")\n",
    "        return None\n",
    "    res = tld.get_tld(site, fix_protocol=True, as_object=True)\n",
    "    return res.tld\n",
    "\n",
    "def extract_domain(site: str, raise_if_invalid=True) -> str:\n",
    "    if not validate(site):\n",
    "        if raise_if_invalid:\n",
    "            raise ValueError(\"Invalid site\")\n",
    "        return None\n",
    "    res = tld.get_tld(site, fix_protocol=True, as_object=True)\n",
    "    return res.fld\n",
    "\n",
    "fld_domain = udf(lambda z: extract_domain(z), StringType())\n",
    "# tld_domain = udf(lambda z: extract_tld(z), StringType())\n",
    "\n",
    "df = df.withColumn(\"fld_domain\", fld_domain(df.host))\n",
    "# df = df.withColumn(\"tld_domain\", tld_domain(df.host))\n",
    "df.filter(col(\"host\") != col(\"fld_domain\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a3fe7",
   "metadata": {},
   "source": [
    "## E-commerce Table vs Model Probabilities Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b425af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/media/greca/HD/Driva/ecommerces_202505050918.csv\")\n",
    "ecommerce_table_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff557017",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df.select(\"dominio\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d722ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df.select(\"host\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df.where(col(\"probabilidade\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_ecommerce_table_df = ecommerce_table_df.where(col(\"probabilidade\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f13631",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_mapping = udf(lambda z: get_interval_range(z), StringType()) \n",
    "\n",
    "not_null_ecommerce_table_df = not_null_ecommerce_table_df.withColumn(\n",
    "    \"interval\",\n",
    "    udf_mapping(not_null_ecommerce_table_df.probabilidade)\n",
    ")\n",
    "\n",
    "total = not_null_ecommerce_table_df.groupBy(\"interval\").count().agg(F.sum(\"count\")).collect()[0][0]\n",
    "not_null_ecommerce_table_df = not_null_ecommerce_table_df.groupBy(\"interval\").count()\n",
    "not_null_ecommerce_table_df = not_null_ecommerce_table_df.withColumn(\"percent\", F.round(not_null_ecommerce_table_df['count']/total, 4))\n",
    "not_null_ecommerce_table_df = not_null_ecommerce_table_df.sort(col(\"percent\").desc())\n",
    "not_null_ecommerce_table_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55647a",
   "metadata": {},
   "source": [
    "### Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/media/greca/HD/Driva/hosts.parquet')\n",
    "unique_domains_whois = df.select(\"host\").distinct().collect()\n",
    "unique_domains_whois = [udw.host for udw in unique_domains_whois]\n",
    "\n",
    "unique_domains_ecomm_table = ecommerce_table_df.select(\"dominio\").distinct().collect()\n",
    "unique_domains_ecomm_table = [udet.dominio for udet in unique_domains_ecomm_table]\n",
    "\n",
    "unique_hosts_ecomm_table = ecommerce_table_df.select(\"host\").distinct().collect()\n",
    "unique_hosts_ecomm_table = [uhet.host for uhet in unique_hosts_ecomm_table]\n",
    "\n",
    "unique_hosts_ecomm_table.extend(unique_domains_ecomm_table)\n",
    "\n",
    "union = list(set(set(unique_domains_whois) | set(unique_hosts_ecomm_table)))\n",
    "intersection = list(set(set(unique_domains_whois) & set(unique_hosts_ecomm_table)))\n",
    "diff_whois_table = list(set(set(unique_domains_whois) - set(unique_hosts_ecomm_table)))\n",
    "diff_table_whois = list(set(set(unique_hosts_ecomm_table) - set(unique_domains_whois)))\n",
    "\n",
    "print(f\"Domains available in both sets: {len(intersection)}\")\n",
    "print(f\"Domains available only in whois set: {len(diff_whois_table)}\")\n",
    "print(f\"Domains available only in ecomm table set: {len(diff_table_whois)}\")\n",
    "print(f\"All domains in both sets: {len(union)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ab9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"/media/greca/HD/Driva/smaller_ecommerce_whois_without_html/*.parquet\")\n",
    "files = sorted(files, key=partition_number)\n",
    "df = spark.read.parquet(*files)\n",
    "\n",
    "# ecommerce_table_df = ecommerce_table_df.withColumnRenamed(\"host\", \"url\")\n",
    "# ecommerce_table_df = ecommerce_table_df.withColumnRenamed(\"dominio\", \"host\")\n",
    "\n",
    "df_only_whois = df.join(\n",
    "    ecommerce_table_df,\n",
    "    df.host ==  ecommerce_table_df.host,\n",
    "    \"leftanti\"\n",
    ")\n",
    "\n",
    "df_only_whois2 = df.join(\n",
    "    ecommerce_table_df,\n",
    "    df.host ==  ecommerce_table_df.dominio,\n",
    "    \"leftanti\"\n",
    ")\n",
    "\n",
    "df_only_whois = df_only_whois.union(df_only_whois2)\n",
    "df_only_whois = df_only_whois.dropDuplicates(subset=[\"host\"])\n",
    "df_only_whois.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_whois = df_only_whois.withColumn(\n",
    "    \"interval\",\n",
    "    udf_mapping(df_only_whois.probability)\n",
    ")\n",
    "\n",
    "total = df_only_whois.groupBy(\"interval\").count().agg(F.sum(\"count\")).collect()[0][0]\n",
    "df_only_whois = df_only_whois.groupBy(\"interval\").count()\n",
    "df_only_whois = df_only_whois.withColumn(\"percent\", F.round(df_only_whois['count']/total, 4))\n",
    "df_only_whois = df_only_whois.sort(col(\"percent\").desc())\n",
    "df_only_whois.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f255624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_table_df = ecommerce_table_df.withColumnRenamed(\"host\", \"host_table_ecomm\")\n",
    "\n",
    "all_df = df.join(\n",
    "    ecommerce_table_df,\n",
    "    df.host ==  ecommerce_table_df.host_table_ecomm,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "all_df2 = df.join(\n",
    "    ecommerce_table_df,\n",
    "    df.host ==  ecommerce_table_df.dominio,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "all_df = all_df.union(all_df2)\n",
    "# all_df = df.join(ecommerce_table_df, on=\"host\", how=\"inner\")\n",
    "all_df = all_df.dropDuplicates(subset=[\"host\"])\n",
    "all_df = all_df.dropDuplicates(subset=[\"dominio\"])\n",
    "all_df = all_df.select(\"host\", \"probabilidade\", \"probability\")\n",
    "all_df = all_df.withColumnRenamed(\"probabilidade\", \"old_model_probability\")\n",
    "all_df = all_df.withColumnRenamed(\"probability\", \"new_model_probability\")\n",
    "all_df = all_df.withColumn(\"difference (%)\", F.round((col(\"new_model_probability\") - col(\"old_model_probability\"))/col(\"old_model_probability\"), 3) * 100)\n",
    "all_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = [-float('inf'), -100, -75, -50, -25, 0, 25, 50, 75, 100, float('inf')]\n",
    "\n",
    "df_final = Bucketizer(\n",
    "    splits=BINS,\n",
    "    inputCol=\"difference (%)\",\n",
    "    outputCol=\"bin\"\n",
    ").transform(all_df)\n",
    "\n",
    "intervals = []\n",
    "\n",
    "for i in range(0, len(BINS)-1):\n",
    "    intervals.append(f\"({BINS[i]}, {BINS[i+1]}]\")\n",
    "\n",
    "mapping = spark.sparkContext.broadcast(intervals)\n",
    "\n",
    "def get_bins(values):\n",
    "    def f(x):\n",
    "        if x is None:\n",
    "            return values[int(0)]\n",
    "        else:\n",
    "            return values[int(x)]\n",
    "    return udf(f)\n",
    "\n",
    "df_final = df_final.withColumn(\"interval\", get_bins(mapping.value)(col(\"bin\")))\n",
    "df_final = df_final.drop(\"bin\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.groupBy(\"interval\").count()\n",
    "total = df_final.agg(F.sum(\"count\")).collect()[0][0]\n",
    "df_final = df_final.withColumn(\"percent\", F.round(df_final['count']/total, 4))\n",
    "df_final = df_final.sort(col(\"percent\").desc())\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_not_null = all_df.where(col(\"old_model_probability\").isNotNull())\n",
    "all_df_not_null = all_df_not_null.withColumn(\"old_model_prediction\", F.when(col(\"old_model_probability\") >= 0.5, True).otherwise(False))\n",
    "all_df_not_null = all_df_not_null.withColumn(\"new_model_prediction\", F.when(col(\"new_model_probability\") >= 0.6, True).otherwise(False))\n",
    "all_df_not_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_not_null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_as_true_before = all_df_not_null.filter((col(\"old_model_prediction\") == True) & (col(\"new_model_prediction\") == False)).count()\n",
    "predicted_as_false_before = all_df_not_null.filter((col(\"old_model_prediction\") == False) & (col(\"new_model_prediction\") == True)).count()\n",
    "same_prediction = all_df_not_null.filter(col(\"old_model_prediction\") == col(\"new_model_prediction\")).count()\n",
    "\n",
    "print(f\"Predicted as True before and now it's False: {predicted_as_true_before}\")\n",
    "print(f\"Predicted as False before and now it's True: {predicted_as_false_before}\")\n",
    "print(f\"Prediction did not change: {same_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a303770",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
