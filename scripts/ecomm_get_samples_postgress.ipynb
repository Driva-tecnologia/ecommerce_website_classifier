{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, rand\n",
    "from pyspark.sql.functions import round as ps_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"App\").setMaster(\"local[*]\")\n",
    "\n",
    "# Habilitar otimizações e configurações adicionais\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.ignore_timezone\", \"true\")\n",
    "\n",
    "# AWS S3 CONNECTION\n",
    "AWS_ENDPOINT_URL = \"https://s3.bhs.io.cloud.ovh.net\"\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_REGION = \"bhs\"\n",
    "\n",
    "conf.set(\"spark.jars\", \"/home/shared/drivers/postgresql-42.7.2.jar\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "# Configurações de tempo e legacy\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\",\"LEGACY\")\n",
    "\n",
    "# Configurações de memória\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"60g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"60g\")\n",
    "\n",
    "# Inicializa o SparkSession com a configuração\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(\"Spark session configurada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = \"driva-db.driva.io\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"postgres\"\n",
    "DB_ECOMM_SCHEMA = \"sites.ecommerces\"\n",
    "DB_TECH_DRIVA = \"sites.ecommerces_tech_driva\"\n",
    "DB_USER = \"\"\n",
    "DB_PASSWORD = \"\"\n",
    "\n",
    "ecomm_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\") \\\n",
    "    .option(\"dbtable\", DB_ECOMM_SCHEMA) \\\n",
    "    .option(\"user\", DB_USER) \\\n",
    "    .option(\"password\", DB_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "tech_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\") \\\n",
    "    .option(\"dbtable\", DB_TECH_DRIVA) \\\n",
    "    .option(\"user\", DB_USER) \\\n",
    "    .option(\"password\", DB_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_df.select(\"tech_driva\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomm_df = ecomm_df.select(\"host\", \"dominio\", \"probabilidade\")\n",
    "merged_df = ecomm_df.join(tech_df, on=\"host\", how=\"inner\")\n",
    "\n",
    "print((merged_df.count(), len(merged_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropDuplicates(subset=[\"dominio\", \"tech_driva\"])\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 6\n",
    "top_tech = merged_df.groupBy(\"tech_driva\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"count\", ps_round((col(\"count\") / merged_df.count()) * 100, 2)) \\\n",
    "    .sort(col(\"count\").desc()) \\\n",
    "    .withColumnRenamed(\"count\", \"percentage (%)\")\n",
    "\n",
    "top_k_tech = top_tech.limit(top_k) \\\n",
    "    .select(\"tech_driva\") \\\n",
    "    .collect()\n",
    "top_k_tech = [tech.tech_driva for tech in top_k_tech]\n",
    "\n",
    "merged_df = merged_df.filter(col(\"tech_driva\").isin(top_k_tech))\n",
    "print((merged_df.count(), len(merged_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tech.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [\n",
    "    (0.0, 0.1),\n",
    "    (0.1, 0.2),\n",
    "    (0.2, 0.3),\n",
    "    (0.3, 0.4),\n",
    "    (0.4, 0.5),\n",
    "    (0.5, 0.6),\n",
    "    (0.6, 0.7),\n",
    "    (0.7, 0.8),\n",
    "    (0.8, 0.9),\n",
    "    (0.9, 1.0)\n",
    "]\n",
    "data = []\n",
    "\n",
    "for (min_interval, max_interval) in intervals:\n",
    "    temp = merged_df.filter((col(\"probabilidade\") > min_interval) & (col(\"probabilidade\") <= max_interval))\n",
    "    row = [\n",
    "        (min_interval, max_interval),\n",
    "        temp.count(),\n",
    "        round((temp.count() / merged_df.count()) * 100, 2)\n",
    "    ]\n",
    "    data.append(row)\n",
    "\n",
    "columns = [\"interval\", \"count\", \"percentage\"]\n",
    "dataframe = spark.createDataFrame(data, columns)\n",
    "dataframe = dataframe.sort(col(\"percentage\").desc())\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prob_ecommerce = merged_df.where((col(\"probabilidade\") > 0.3) & (col(\"probabilidade\") <= 0.6))#.dropDuplicates(subset=[\"dominio\"])\n",
    "high_prob_ecommerce = high_prob_ecommerce.withColumn(\"predicted_as_ecommerce\", lit(True))\n",
    "high_prob_ecommerce.show()\n",
    "\n",
    "print((high_prob_ecommerce.count(), len(high_prob_ecommerce.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_prob_ecommerce = merged_df.where((col(\"probabilidade\") > 0.1) & (col(\"probabilidade\") <= 0.3))#.dropDuplicates(subset=[\"dominio\"])\n",
    "low_prob_ecommerce = low_prob_ecommerce.withColumn(\"predicted_as_ecommerce\", lit(False))\n",
    "low_prob_ecommerce.show()\n",
    "\n",
    "print((low_prob_ecommerce.count(), len(low_prob_ecommerce.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df, n_samples, seed):\n",
    "    df_copy = spark.createDataFrame(df.rdd.map(lambda x: x), schema=df.schema)\n",
    "    new_df = spark.createDataFrame(\n",
    "        spark.sparkContext.emptyRDD(),\n",
    "        schema=df.schema,\n",
    "    )\n",
    "    new_top_tech = df.groupBy(\"tech_driva\") \\\n",
    "        .count() \\\n",
    "        .withColumn(\"count\", ps_round((col(\"count\") / df.count()) * 100, 2)) \\\n",
    "        .withColumnRenamed(\"count\", \"percentage (%)\")\n",
    "        \n",
    "    for tech_driva, pct in new_top_tech.rdd.collect():\n",
    "        tech_samples = (int(pct * n_samples) + 1) // 100\n",
    "\n",
    "        temp = df_copy.filter(col(\"tech_driva\") == tech_driva)\n",
    "        temp = spark.createDataFrame(\n",
    "            temp.rdd.takeSample(\n",
    "                withReplacement=False,\n",
    "                num=tech_samples,\n",
    "                seed=seed,\n",
    "            ),\n",
    "            schema=df.schema,\n",
    "        )\n",
    "\n",
    "        unique_domains = temp.select(\"tech_driva\").distinct().collect()\n",
    "        unique_domains = [ud.tech_driva for ud in unique_domains]\n",
    "        df_copy = df_copy.filter(col(\"tech_driva\").isin(unique_domains) == False)\n",
    "\n",
    "        new_df = new_df.union(temp)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "seed = 42\n",
    "n_samples = 5000\n",
    "\n",
    "high_prob_ecommerce_filtered = filter_dataframe(\n",
    "    df=high_prob_ecommerce,\n",
    "    n_samples=n_samples,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "print((high_prob_ecommerce_filtered.count(), len(high_prob_ecommerce_filtered.columns)))\n",
    "\n",
    "low_prob_ecommerce_filtered = filter_dataframe(\n",
    "    df=low_prob_ecommerce,\n",
    "    n_samples=n_samples,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "print((low_prob_ecommerce_filtered.count(), len(low_prob_ecommerce_filtered.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = high_prob_ecommerce_filtered.union(low_prob_ecommerce_filtered)\n",
    "full_df = full_df.dropDuplicates(subset=[\"dominio\"])\n",
    "full_df = full_df.orderBy(rand())\n",
    "\n",
    "print(full_df.show())\n",
    "print((full_df.count(), len(full_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df.write.save(\"s3a://drivalake/trusted/sites/ecommerces/dados_treino/mid_prob_data_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
