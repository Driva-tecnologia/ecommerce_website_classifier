{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and df reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/training_data.parquet').rename(columns={'website': 'domain', 'is_true_ecommerce': 'true_ecommerce'})\n",
    "# df = pd.read_parquet('../data/noisy_training_data.parquet')\n",
    "df['html'] = df['html'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_df = pd.read_parquet('../data/extra_training_data.parquet').rename(columns={'website': 'domain', 'is_true_ecommerce': 'true_ecommerce'})\n",
    "# # df = pd.read_parquet('../data/noisy_training_data.parquet')\n",
    "# extra_df['html'] = extra_df['html'].astype(str)\n",
    "\n",
    "# df = pd.concat([df, extra_df], axis=0, ignore_index=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_ecommerce'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'domain',\n",
    "            'html',\n",
    "            ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '') & \n",
    "                                (dataframe['domain'].str.endswith('.br'))]\n",
    "    \n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates(subset=[\"domain\"])\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "        nulls = dataframe['domain'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'domain' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['domain'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        # pegar somente o body do HTML\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_html_for_how_many_values(text):\n",
    "    try:              \n",
    "        regex_valores = re.compile(r'\\d+(?:\\.\\d{3})*(?:,\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        valores = regex_valores.findall(text)\n",
    "        return len(valores)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for values.\\nError:\\n' + str(e))\n",
    "\n",
    "def get_html_body(html_str):\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_links(row):\n",
    "    try:\n",
    "        from scrapy.http import HtmlResponse\n",
    "        from scrapy.linkextractors import LinkExtractor\n",
    "        \n",
    "        # Definindo os parâmetros do LinkExtractor\n",
    "        allowed_domains = []\n",
    "        tags = ['a', 'area']\n",
    "        attrs = ['href',]\n",
    "        link_extractor = LinkExtractor(allow_domains=allowed_domains, tags=tags, attrs=attrs, unique=False)\n",
    "\n",
    "        html = row['html']\n",
    "        domain = row['domain']\n",
    "        response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "        links = link_extractor.extract_links(response)\n",
    "\n",
    "        return [link.url for link in links]\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while searching for links in HTML.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoreference_links_from_html(row):\n",
    "    from scrapy.http import HtmlResponse\n",
    "    from scrapy.linkextractors import LinkExtractor\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "    \n",
    "    html = row['html']\n",
    "    domain = row['domain']\n",
    "    parsed_domain = urlparse(domain).netloc  # Parse the domain to get the netloc part\n",
    "\n",
    "    response = HtmlResponse(url=domain, body=html, encoding='utf-8')\n",
    "    link_extractor = LinkExtractor(tags=['a', 'area'], attrs=['href'], unique=False)\n",
    "    links = link_extractor.extract_links(response)\n",
    "\n",
    "    autoreference_links = []\n",
    "    for link in links:\n",
    "        link_url = urlparse(link.url)\n",
    "        # Check if the link is a relative link or it belongs to the same domain\n",
    "        if not link_url.netloc or link_url.netloc == parsed_domain:\n",
    "            # Resolve relative link to absolute URL\n",
    "            full_url = urljoin(domain, link.url)\n",
    "            autoreference_links.append(full_url)\n",
    "\n",
    "    return autoreference_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataframe(dataframe, aditional_columns):\n",
    "    try:\n",
    "        # Colunas necessárias para o modelo\n",
    "        feature_columns = []\n",
    "\n",
    "        for columns in aditional_columns:\n",
    "            feature_columns += columns\n",
    "        df_features = dataframe.loc[:, feature_columns]\n",
    "        \n",
    "        return df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while trying to build features DataFrame.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()    \n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)    \n",
    "        dataframe.loc[:, 'tokens'] = dataframe.loc[:, 'html'].apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "        # dataframe.loc[:, 'html_size'] = dataframe.loc[:, 'html'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens'] = dataframe.loc[:, 'tokens'].apply(len)\n",
    "        # dataframe.loc[:, 'qntd_tokens_unicos'] = dataframe.loc[:, 'tokens'].apply(lambda x: len(set(x)))\n",
    "\n",
    "        # dataframe.loc[:, 'autoreference_links'] = dataframe.apply(get_autoreference_links_from_html, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_autoreference_links'] = dataframe.loc[:, 'autoreference_links'].apply(len)\n",
    "\n",
    "        # dataframe.loc[:, 'links'] = dataframe.apply(get_html_links, axis=1)\n",
    "        # dataframe.loc[:, 'qntd_links'] = dataframe.loc[:, 'links'].apply(len)\n",
    "\n",
    "        dataframe.loc[:, 'processed_cnpjs'] = dataframe.loc[:, 'html'].apply(extract_and_process_cnpjs)\n",
    "        dataframe.loc[:, 'has_cnpj'] = dataframe.loc[:, 'processed_cnpjs'].apply(bool)\n",
    "\n",
    "        dataframe.loc[:, 'count_prices'] = html_body.apply(process_html_for_how_many_prices)\n",
    "        # #dataframe.loc[:, 'count_prices'] = dataframe.loc[:, 'prices'].apply(len)\n",
    "        dataframe['has_prices'] = dataframe['count_prices'] > 1\n",
    "\n",
    "        # dataframe.loc[:, 'count_values'] = html_body.apply(process_html_for_how_many_values)\n",
    "        # #dataframe.loc[:, 'count_values'] = dataframe.loc[:, 'values'].apply(len)\n",
    "        \n",
    "        # aditional_columns = [\n",
    "        #     ['true_ecommerce']\n",
    "        # ]\n",
    "        # df_features = get_features_dataframe(dataframe, aditional_columns)\n",
    "\n",
    "        return dataframe#, df_features\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(score, confusion_matrix, classification_report, model_card):\n",
    "    \n",
    "    # Gera o heatmap da confusion matrix\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt=\"d\", \n",
    "                linewidths=.5, \n",
    "                square = True, \n",
    "                cmap = 'Blues', \n",
    "                annot_kws={\"size\": 16}, \n",
    "                xticklabels=['non_ecom', 'ecom'], \n",
    "                yticklabels=['non_ecom', 'ecom'])\n",
    "\n",
    "    plt.xticks(rotation='horizontal', fontsize=16)\n",
    "    plt.yticks(rotation='horizontal', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', size=20)\n",
    "    plt.ylabel('Actual Label', size=20)\n",
    "\n",
    "    title = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    plt.title(title, size = 20)\n",
    "\n",
    "    # Mostra o classification report e o heatmap\n",
    "    pprint(classification_report)\n",
    "    plt.show()\n",
    "\n",
    "    model_card['accuracy_best'] = round(classification_report['accuracy'], 4)\n",
    "    model_card['precision_macro_best'] = round(classification_report['macro avg']['precision'], 4)\n",
    "    model_card['recall_macro_best'] = round(classification_report['macro avg']['recall'], 4)\n",
    "    model_card['f1_macro_best'] = round(classification_report['macro avg']['f1-score'], 4)\n",
    "    model_card['support_0_best'] = classification_report['0']['support']\n",
    "    model_card['support_1_best'] = classification_report['1']['support']\n",
    "\n",
    "    return model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_report(cross_validate_results, model_card):\n",
    "\n",
    "    # métricas dos modelos gerados no cross validation\n",
    "    print('accuracy:\\t', cross_validate_results['test_accuracy'], ' \\tmean: ', cross_validate_results['test_accuracy'].mean())\n",
    "    print('precision:\\t', cross_validate_results['test_precision'], ' \\tmean: ', cross_validate_results['test_precision'].mean())\n",
    "    print('recall:\\t\\t', cross_validate_results['test_recall'], ' \\tmean: ', cross_validate_results['test_recall'].mean())\n",
    "    print('f1:\\t\\t', cross_validate_results['test_f1'], ' \\tmean: ', cross_validate_results['test_f1'].mean())\n",
    "    print('fit_time:\\t', cross_validate_results['fit_time'], ' \\tmean: ', cross_validate_results['fit_time'].mean())\n",
    "    print('score_time:\\t', cross_validate_results['score_time'], ' \\tmean: ', cross_validate_results['score_time'].mean())\n",
    "\n",
    "    max_f1_pos = list(cross_validate_results['test_f1']).index(max(cross_validate_results['test_f1']))\n",
    "    best_estimator = cross_validate_results['estimator'][max_f1_pos]\n",
    "    best_indices = {\n",
    "        'train': cross_validate_results['indices']['train'][max_f1_pos],\n",
    "        'test': cross_validate_results['indices']['test'][max_f1_pos]\n",
    "    }\n",
    "\n",
    "    model_card['accuracy_mean'] = round(cross_validate_results['test_accuracy'].mean(), 4)\n",
    "    model_card['precision_mean'] = round(cross_validate_results['test_precision'].mean(), 4)\n",
    "    model_card['recall_mean'] = round(cross_validate_results['test_recall'].mean(), 4)\n",
    "    model_card['f1_mean'] = round(cross_validate_results['test_f1'].mean(), 4)\n",
    "    model_card['fit_time_mean'] = round(cross_validate_results['fit_time'].mean(), 4)\n",
    "    model_card['score_time_mean'] = round(cross_validate_results['score_time'].mean(), 4)\n",
    "\n",
    "    \n",
    "    return best_estimator, best_indices, model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectorizer_model(vectorizer, model, model_card):\n",
    "    try:\n",
    "        file_name = model_card['scope'] + '_' + model_card['vectorizer'] + '_' + model_card['model'] + '_' + \\\n",
    "            str(model_card['word_reduction']) + '_' + str(model_card['kfold_splits']) + '_' + str(model_card['kfold_shuffle']) + '_' + \\\n",
    "                str(model_card['kfold_random_state']) + '_' + str(model_card['vectorizer_max_features']) + '_' + model_card['dataset']\n",
    "\n",
    "        model_card['vectorizer_file_name'] = 'VECTORIZER_v1_' + file_name + '.pkl'\n",
    "        model_card['model_file_name'] = 'MODEL_v1_' + file_name + '.pkl'\n",
    "\n",
    "        with open('../models/' + model_card['vectorizer_file_name'], 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "        with open('../models/' + model_card['model_file_name'], 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('An error ocurred while trying to save the model. Error: ' + str(e))\n",
    "        print(model_card)\n",
    "\n",
    "    return model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = {\n",
    "    'scope': 'ecommerce',\n",
    "    'vectorizer': '',\n",
    "    'model': '',\n",
    "    'word_reduction': 'custom_lemmatizer', \n",
    "    'kfold_splits': 3,\n",
    "    'kfold_shuffle': True,\n",
    "    'kfold_random_state': 42,\n",
    "    'vectorizer_max_features': 1000, # None ou INTeger -> testado: None, 50, 100, 200, 500, 1000, 1500, 2000\n",
    "    'dataset': 'training_data', \n",
    "    'accuracy_mean': '',\n",
    "    'precision_mean': '',\n",
    "    'recall_mean': '',\n",
    "    'f1_mean': '',\n",
    "    'fit_time_mean': '',\n",
    "    'score_time_mean': '',\n",
    "    'accuracy_best': '',\n",
    "    'precision_macro_best': '',\n",
    "    'recall_macro_best': '',\n",
    "    'f1_macro_best': '',\n",
    "    'support_0_best': '',\n",
    "    'support_1_best': '',\n",
    "    'vectorizer_file_name': '',\n",
    "    'model_file_name': ''\n",
    "}\n",
    "\n",
    "# metricas utilizadas pela validação cruzada\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score, normalize=True), \n",
    "    'precision': make_scorer(precision_score, average='macro', zero_division=0), \n",
    "    'recall': make_scorer(recall_score, average='macro'), \n",
    "    'f1': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "\n",
    "# CV splitter com StratifiedKFold, para manter proporções de exemplos de cada classe target\n",
    "stratified_kfold = StratifiedKFold(n_splits=model_card['kfold_splits'], shuffle=model_card['kfold_shuffle'], random_state=model_card['kfold_random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive-Bayes com TF-IDF\n",
    "mnb_tfidf_model_card = model_card.copy()\n",
    "mnb_tfidf_model_card['vectorizer'] = 'tfidf_vectorizer'\n",
    "mnb_tfidf_model_card['model'] = 'multinomial_nb'\n",
    "\n",
    "cv_tfidf_vectorizer = TfidfVectorizer(max_features=model_card['vectorizer_max_features'])\n",
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "tfidf_matrix = cv_tfidf_vectorizer.fit_transform(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = cv_tfidf_vectorizer.idf_\n",
    "feature_names = cv_tfidf_vectorizer.get_feature_names_out()\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'idf': idf_values})\n",
    "feature_importances = feature_importances.sort_values(by='idf', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['html_size', 'qntd_tokens', 'qntd_tokens_unicos', 'qntd_autoreference_links','qntd_links', 'has_cnpj', 'count_prices', 'count_values', 'has_prices']\n",
    "features = ['has_cnpj', 'has_prices']\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=cv_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "other_features = df[features]\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "df_y = df['true_ecommerce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(other_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# lgrg_tfidf = LogisticRegression(solver='liblinear')\n",
    "clf = MultinomialNB()\n",
    "# lgrg_tfidf = LogisticRegression(solver='lbfgs') # lbfgs é melhor, mas max_iter tava sendo mt alta, n tava convergindo. Testar os hyperparams\n",
    "# cross validation\n",
    "cv_results = cross_validate(estimator=clf, X=features_df, y=df_y, \n",
    "                            cv=stratified_kfold, scoring=scoring_metrics,\n",
    "                            return_estimator=True, return_indices=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate report\n",
    "best_estimator, best_indices, mnb_tfidf_model_card = cross_validate_report(cv_results, mnb_tfidf_model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# best model report\n",
    "best_indices_test_X = features_df.iloc[best_indices['test']]\n",
    "best_indices_test_Y = df_y.iloc[best_indices['test']]\n",
    "\n",
    "model_predictions_prob = best_estimator.predict_proba(best_indices_test_X)\n",
    "y_probs_0, y_probs_1 = zip(*model_predictions_prob)\n",
    "y_probs_0 = list(y_probs_0)\n",
    "y_probs_1 = list(y_probs_1)\n",
    "best_estimator_predictions = np.asarray(list(map(lambda x: int(x >= THRESHOLD), y_probs_1)))\n",
    "\n",
    "# best_estimator_score = best_estimator.score(X=best_indices_test_X, y=best_indices_test_Y)\n",
    "best_estimator_score = accuracy_score(y_true=best_indices_test_Y, y_pred=best_estimator_predictions)\n",
    "best_estimator_score_f1 = f1_score(y_true=best_indices_test_Y, y_pred=best_estimator_predictions)\n",
    "\n",
    "best_estimator_cmatrix = confusion_matrix(y_pred=best_estimator_predictions, y_true=best_indices_test_Y)\n",
    "best_estimator_creport = classification_report(y_pred=best_estimator_predictions, y_true=best_indices_test_Y, zero_division=0, output_dict=True)\n",
    "\n",
    "mnb_tfidf_model_card = model_report(best_estimator_score, best_estimator_cmatrix, best_estimator_creport, mnb_tfidf_model_card)\n",
    "\n",
    "# save model\n",
    "mnb_tfidf_model_card = save_vectorizer_model(cv_tfidf_vectorizer, clf, mnb_tfidf_model_card)\n",
    "\n",
    "print(f\"F1-Score: {round(best_estimator_score_f1, 4)}\")\n",
    "print(mnb_tfidf_model_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(best_estimator_cmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando com o modelo atual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "last_model_path = \"../models/MODEL_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl\"\n",
    "last_vectorizer_path = \"../models/VECTORIZER_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl\"\n",
    "\n",
    "with open(last_vectorizer_path, \"rb\") as f:\n",
    "    last_version_vectorizer = pickle.load(f)\n",
    "\n",
    "with open(last_model_path, \"rb\") as f:\n",
    "    last_version_model = pickle.load(f)\n",
    "\n",
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "tfidf_matrix = last_version_vectorizer.transform(token_strings)\n",
    "\n",
    "idf_values = last_version_vectorizer.idf_\n",
    "\n",
    "feature_names = last_version_vectorizer.get_feature_names_out()\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'idf': idf_values})\n",
    "feature_importances = feature_importances.sort_values(by='idf', ascending=False)\n",
    "\n",
    "# features = ['html_size', 'qntd_tokens', 'qntd_tokens_unicos', 'qntd_autoreference_links','qntd_links', 'has_cnpj', 'count_prices', 'count_values', 'has_prices']\n",
    "features = ['has_cnpj', 'has_prices']\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=last_version_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "other_features = df[features]\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "df_y = df['true_ecommerce']\n",
    "\n",
    "# best model report\n",
    "best_indices_test_X = features_df.iloc[best_indices['test']]\n",
    "best_indices_test_Y = df_y.iloc[best_indices['test']]\n",
    "\n",
    "model_predictions_prob = last_version_model.predict_proba(best_indices_test_X)\n",
    "y_probs_0, y_probs_1 = zip(*model_predictions_prob)\n",
    "y_probs_0 = list(y_probs_0)\n",
    "y_probs_1 = list(y_probs_1)\n",
    "current_estimator_predictions = np.asarray(list(map(lambda x: int(x >= THRESHOLD), y_probs_1)))\n",
    "\n",
    "# best_estimator_score = best_estimator.score(X=best_indices_test_X, y=best_indices_test_Y)\n",
    "current_estimator_score = accuracy_score(y_true=best_indices_test_Y, y_pred=current_estimator_predictions)\n",
    "current_estimator_score_f1 = f1_score(y_true=best_indices_test_Y, y_pred=current_estimator_predictions)\n",
    "\n",
    "current_estimator_cmatrix = confusion_matrix(y_pred=current_estimator_predictions, y_true=best_indices_test_Y)\n",
    "current_estimator_creport = classification_report(y_pred=current_estimator_predictions, y_true=best_indices_test_Y, zero_division=0, output_dict=True)\n",
    "\n",
    "print(f\"F1-Score: {round(current_estimator_score_f1, 4)}\")\n",
    "\n",
    "# Gera o heatmap da confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(current_estimator_cmatrix, \n",
    "            annot=True, \n",
    "            fmt=\"d\", \n",
    "            linewidths=.5, \n",
    "            square = True, \n",
    "            cmap = 'Blues', \n",
    "            annot_kws={\"size\": 16}, \n",
    "            xticklabels=['non_ecom', 'ecom'], \n",
    "            yticklabels=['non_ecom', 'ecom'])\n",
    "\n",
    "plt.xticks(rotation='horizontal', fontsize=16)\n",
    "plt.yticks(rotation='horizontal', fontsize=16)\n",
    "plt.xlabel('Predicted Label', size=20)\n",
    "plt.ylabel('Actual Label', size=20)\n",
    "\n",
    "title = 'Accuracy Score: {0:.4f}'.format(current_estimator_score)\n",
    "plt.title(title, size = 20)\n",
    "\n",
    "# Mostra o classification report e o heatmap\n",
    "pprint(current_estimator_creport)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(current_estimator_cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = df.iloc[best_indices_test_Y.index.tolist()].copy()\n",
    "analysis_df[\"prediction\"] = best_estimator_predictions.tolist()\n",
    "analysis_df = analysis_df[[\"domain\", \"html\", \"tokens\", \"true_ecommerce\", \"prediction\"]]\n",
    "analysis_df = analysis_df[(analysis_df[\"true_ecommerce\"] == 0) & (analysis_df[\"prediction\"] == 1)]\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(features_df, df_y)\n",
    "mnb_tfidf_model_card = save_vectorizer_model(cv_tfidf_vectorizer, clf, mnb_tfidf_model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on validated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/countries_predictions_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['domain','html']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "tfidf_matrix = cv_tfidf_vectorizer.transform(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['has_cnpj', 'has_prices']\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=cv_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "other_features = df[features]\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_with_domain(X, estimator, vectorizer, threshold=0.5):\n",
    "    model_predictions_prob = estimator.predict_proba(X)\n",
    "\n",
    "    y_probs_0, y_probs_1 = zip(*model_predictions_prob)\n",
    "    y_probs_0 = list(y_probs_0)\n",
    "    y_probs_1 = list(y_probs_1)\n",
    "    y_preds = list(map(lambda x: int(x >= threshold), y_probs_1))\n",
    "\n",
    "    return y_preds, y_probs_0, y_probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain(features_df, clf, cv_tfidf_vectorizer, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_0_prob'] = y_probs_0\n",
    "df['pred_1_prob'] = y_probs_1\n",
    "df['prediction'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/countries_predictions_sample_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
