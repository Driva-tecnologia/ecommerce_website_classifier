{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import FloatType, BooleanType, StructField, StructType, DoubleType, ArrayType\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_ENDPOINT_URL = os.getenv('AWS_ENDPOINT_URL')\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/darrazao/git/accounting_website_classifier/venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/darrazao/.ivy2/cache\n",
      "The jars for the packages stored in: /home/darrazao/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-46870995-4730-440c-abd1-cd4b04c1b25f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 164ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-46870995-4730-440c-abd1-cd4b04c1b25f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "24/06/24 20:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/24 20:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Spark com S3\").setMaster(\"local[*]\")\n",
    "\n",
    "conf.set(\"spark.driver.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.memory\", \"70g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"70g\")\n",
    "\n",
    "# conf.set(\"spark.driver.cores\", \"20\")\n",
    "# conf.set(\"spark.executor.cores\", \"20\")\n",
    "\n",
    "# conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "# conf.set(\"spark.memory.offHeap.size\", \"20g\")\n",
    "\n",
    "# conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "# conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"2048\") \n",
    "conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Inicialize o cliente boto3 para listar os objetos na pasta S3\n",
    "s3 = boto3.client('s3', endpoint_url='https://s3.bhs.io.cloud.ovh.net')\n",
    "bucket_name = 'drivalake'\n",
    "prefix = 'sites/bronze/spiderwebv4/countries_'\n",
    "\n",
    "# Função para listar todos os arquivos no bucket/prefix\n",
    "def list_s3_files(bucket, prefix):\n",
    "    files = []\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    for content in response.get('Contents', []):\n",
    "        files.append(content['Key'])\n",
    "    while 'NextContinuationToken' in response:\n",
    "        continuation_token = response['NextContinuationToken']\n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "        for content in response.get('Contents',  []):\n",
    "            files.append(content['Key'])\n",
    "    return files\n",
    "\n",
    "# Listar todos os arquivos\n",
    "files = list_s3_files(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'domain',\n",
    "            'html',\n",
    "            ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '') & \n",
    "                                (dataframe['domain'].str.endswith('.br'))]\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates()\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "    \n",
    "        nulls = dataframe['domain'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'domain' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['domain'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        # if os.path.exists(file_name):\n",
    "        #     os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_body(html_str):\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        # pegar somente o body do HTML\n",
    "        text = get_html_body(html_text)\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_how_many_prices(text):\n",
    "    import re\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()        \n",
    "        dataframe.loc[:, 'tokens'] = dataframe.loc[:, 'html'].apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "\n",
    "        dataframe.loc[:, 'processed_cnpjs'] = dataframe.loc[:, 'html'].apply(extract_and_process_cnpjs)\n",
    "        dataframe.loc[:, 'has_cnpj'] = dataframe.loc[:, 'processed_cnpjs'].apply(bool)\n",
    "\n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)\n",
    "        dataframe.loc[:, 'count_prices'] = html_body.apply(process_html_for_how_many_prices)\n",
    "        dataframe['has_prices'] = dataframe['count_prices'] > 1\n",
    "\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_with_domain(domains: list, HTML_raw: list, estimator, vectorizer):\n",
    "\n",
    "    df = pd.DataFrame({'domain': domains, 'html': HTML_raw})\n",
    "    df = generate_features(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "    tfidf_matrix = vectorizer.transform(token_strings)\n",
    "\n",
    "    features = ['has_cnpj', 'has_prices']\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    other_features = df[features]\n",
    "    features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "    \n",
    "    model_predictions_prob = estimator.predict_proba(features_df)\n",
    "\n",
    "    y_probs_0 = []\n",
    "    y_probs_1 = []\n",
    "    y_preds = []\n",
    "\n",
    "    for prob_tuple in model_predictions_prob:\n",
    "        y_probs_0.append(prob_tuple[0])\n",
    "        y_probs_1.append(prob_tuple[1])\n",
    "\n",
    "        if prob_tuple[1] >= 0.5:\n",
    "            y_preds.append(1)\n",
    "        else:\n",
    "                y_preds.append(0)\n",
    "\n",
    "    return y_preds, y_probs_0, y_probs_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9/127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 20:42:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "loading model...\n",
      "predicting...\n",
      "writing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current mem limits: -1 of max -1                                  (0 + 40) / 45]\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 75161927680 of max 75161927680\n",
      "\n",
      "/tmp/ipykernel_2676859/868273156.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "/tmp/ipykernel_2676859/868273156.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "Current mem limits: 75161927680 of max 75161927680                (1 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (1 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (2 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680                (4 + 40) / 45]\n",
      "\n",
      "Current mem limits: 75161927680 of max 75161927680\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10/127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "loading model...\n",
      "predicting...\n",
      "writing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 40) / 41]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/darrazao/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/darrazao/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 5:====>                                                    (3 + 38) / 41]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriting...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/ecommerce/countries_filtered_with_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mdf_with_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/git/accounting_website_classifier/venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================================>        (35 + 6) / 41]\r"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]\n",
    "\n",
    "# Carregar e processar cada parte separadamente\n",
    "for i, batch in enumerate(batches):\n",
    "    if i < 9: continue # 74+\n",
    "    \n",
    "    print(f\"Processing batch {i+1}/{len(batches)}\")\n",
    "    file_paths = [f\"s3a://{bucket_name}/{file}\" for file in batch]\n",
    "    df_spider_br = spark.read.parquet(*file_paths)\n",
    "    \n",
    "    # Fazer o processamento necessário com df_batch\n",
    "    print('preprocessing...')\n",
    "    df_spider_br = df_spider_br.select('domain', 'html', 'status')\n",
    "    df_spider_br = df_spider_br.withColumn('html', col('html').cast('string'))\n",
    "    df_spider_br = df_spider_br.filter((col('status') == 200.0) & (col('html') != '[]') & (col('html') != '') & (col('domain').endswith('.br')))\n",
    "    df_spider_br = df_spider_br.select('domain', 'html')\n",
    "    df_spider_br = df_spider_br.dropDuplicates()\n",
    "\n",
    "    ###########################################################3\n",
    "\n",
    "    print('loading model...')\n",
    "    # Open picked model\n",
    "    serialized_model = open('../models/MODEL_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\")\n",
    "    model = pickle.load(serialized_model)\n",
    "    serialized_model.close()\n",
    "    # Open picked vectorizer\n",
    "    serialized_vectorizer = open('../models/VECTORIZER_ecommerce_tfidf_vectorizer_mnb_custom_lemmatizer_3_True_42_1000_spiderwebv4_dataset_html.pkl', \"rb\")\n",
    "    vectorizer = pickle.load(serialized_vectorizer)\n",
    "    serialized_vectorizer.close()\n",
    "\n",
    "    # Broadcast model to spark executors\n",
    "    spark.sparkContext.broadcast(model)\n",
    "    spark.sparkContext.broadcast(vectorizer)\n",
    "\n",
    "    # prediction method\n",
    "    def predictor(domain, html):\n",
    "        y_preds, y_probs_0, y_probs_1 = predict_proba_with_domain([domain], [html], model, vectorizer)\n",
    "        return (float(y_probs_1[0]), bool(y_preds[0]))\n",
    "\n",
    "    result_schema = StructType([\n",
    "        StructField(\"probability\", DoubleType()),\n",
    "        StructField(\"prediction\", BooleanType())\n",
    "    ])\n",
    "\n",
    "    #register python method as spark UDF\n",
    "    udf_predictor = udf(predictor, result_schema)\n",
    "\n",
    "    ###########################################################\n",
    "    print('predicting...')\n",
    "    df_with_predictions = df_spider_br.withColumn('results', udf_predictor(df_spider_br.domain, df_spider_br.html))\n",
    "    \n",
    "    # Criar colunas separadas para probability e prediction\n",
    "    df_with_predictions = df_with_predictions.withColumn(\"probability\", col(\"results.probability\")) \\\n",
    "                                             .withColumn(\"prediction\", col(\"results.prediction\")) \\\n",
    "                                             .drop('results')\n",
    "\n",
    "\n",
    "    print('writing...')\n",
    "    file_path = '../data/countries_filtered_with_predictions'\n",
    "    df_with_predictions.write.parquet(file_path, mode='append')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 21:01:54 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 40.0 in stage 5.0 (TID 128)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:146)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:144)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/24 21:01:54 ERROR Executor: Exception in task 40.0 in stage 5.0 (TID 128): /tmp/blockmgr-e19dc3e8-38f3-4c4b-bd91-2423233145d5/08/shuffle_1_128_0.index.c14b1da0-5c23-4c7a-a0a7-fb4e8e91bc72 (No such file or directory)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spider_br.write.parquet('./data/spider_br/brazil_filtered.parquet', mode='error')\n",
    "# df_test = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')\n",
    "# df_spider_br = spark.read.parquet('./data/spider_br/brazil_filtered.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
